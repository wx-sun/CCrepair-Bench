#è¿™ä¸ªæ–‡ä»¶çš„åŠŸèƒ½æ˜¯ç”¨æ¥è°ƒç”¨llmæ¨¡å‹ï¼Œè¯„ä»·æ¨¡å‹èƒ½å¦ç”Ÿæˆè§£å†³ç¼–è¯‘é”™è¯¯çš„ä»£ç ç‰‡æ®µï¼Œè¯„åˆ¤è¦æ±‚æ˜¯llmç”Ÿæˆçš„ä»£ç èƒ½å¤Ÿé€šè¿‡gccç¼–è¯‘ï¼Œå¹¶ä¸”èƒ½å¤Ÿè§£å†³ç¼–è¯‘é”™è¯¯ã€‚è¯·ç”¨å¹¶å‘çš„æ–¹å¼è°ƒç”¨æ¨¡å‹ã€‚

import json
import requests
from tqdm import tqdm
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import subprocess
import tempfile
import os
from typing import Dict, List, Tuple
import statistics

'''
é”™è¯¯ç±»å‹: {error_type}
é”™è¯¯æè¿°: {error_type_detail}
llm_examples_dataä¸­çš„é”™è¯¯æºç :{error_example_llm_code}
error_base_dataä¸­çš„é”™è¯¯æºç :{error_example_code}

'''

class LLMJudge:
    def __init__(self, base_url, headers):
        self.base_url = base_url
        self.headers = headers
        self.judge_prompt_template = """ä½ æ˜¯ä¸€ä¸ªC++ç¼–ç¨‹ä¸“å®¶å’Œä»£ç è´¨é‡è¯„å®¡å‘˜ã€‚è¯·è¯„ä¼°ä¸€ä¸ªä»£ç ä¿®å¤æ˜¯å¦çœŸæ­£è§£å†³äº†ç¼–è¯‘é”™è¯¯ï¼Œè€Œä¸æ˜¯ç®€å•åœ°åˆ é™¤äº†ç›¸å…³ä»£ç ã€‚

åŸå§‹é”™è¯¯ä¿¡æ¯ï¼š
é”™è¯¯ç±»å‹: {error_type}
é”™è¯¯æè¿°: {error_type_detail}

åŸå§‹æœ‰é—®é¢˜çš„ä»£ç ï¼š
```cpp
{original_code}
```

ä¿®å¤åçš„ä»£ç ï¼š
```cpp
{fixed_code}
```

è¯·åˆ†æä¿®å¤åçš„ä»£ç æ˜¯å¦çœŸæ­£è§£å†³äº†ç¼–è¯‘é”™è¯¯ï¼Œè¯„ä¼°æ ‡å‡†å¦‚ä¸‹ï¼š

1. **çœŸæ­£ä¿®å¤**: ä»£ç ä¿®æ­£äº†è¯­æ³•é”™è¯¯ã€æ·»åŠ äº†ç¼ºå¤±çš„å¤´æ–‡ä»¶/å£°æ˜ã€ä¿®å¤äº†ç±»å‹ä¸åŒ¹é…ç­‰é—®é¢˜ï¼Œä¿æŒäº†åŸæœ‰åŠŸèƒ½é€»è¾‘
2. **ç®€å•åˆ é™¤**: é€šè¿‡åˆ é™¤å‡ºé”™çš„ä»£ç è¡Œã€å‡½æ•°æˆ–åŠŸèƒ½æ¥é¿å…ç¼–è¯‘é”™è¯¯ï¼Œä½†ä¸¢å¤±äº†åŸæœ‰åŠŸèƒ½
3. **è¿‡åº¦ä¿®æ”¹**: å¤§å¹…æ”¹å˜äº†åŸæœ‰é€»è¾‘æˆ–æ·»åŠ äº†ä¸å¿…è¦çš„ä»£ç 
4. **æ— æ•ˆä¿®å¤**: ä¿®å¤ä¸æ­£ç¡®æˆ–å¯èƒ½å¼•å…¥æ–°çš„é—®é¢˜

è¯·ç»™å‡ºä½ çš„åˆ¤æ–­ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
åˆ¤æ–­ç»“æœ: [çœŸæ­£ä¿®å¤/ç®€å•åˆ é™¤/è¿‡åº¦ä¿®æ”¹/æ— æ•ˆä¿®å¤]
ç½®ä¿¡åº¦: [0-100]
ç†ç”±: [è¯¦ç»†è¯´æ˜ä½ çš„åˆ¤æ–­ä¾æ®ï¼ŒåŒ…æ‹¬å…·ä½“çš„ä¿®å¤ç‚¹åˆ†æ]"""

    def get_llm_response(self, prompt, temperature=0.1):
        """è°ƒç”¨LLM APIè·å–å“åº”"""
        payload = {
            "model": "test",
            "max_tokens": 4096,
            "temperature": temperature,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªC++ç¼–ç¨‹ä¸“å®¶å’Œä»£ç è´¨é‡è¯„å®¡å‘˜ã€‚"},
                {"role": "user", "content": prompt}
            ],
            "stream": False
        }

        try:
            response = requests.request("POST", self.base_url, json=payload, headers=self.headers, verify=False)
            response.raise_for_status()
            content = json.loads(response.text)['choices'][0]['message']['content']
            return content
        except Exception as e:
            print(f"è£åˆ¤APIè°ƒç”¨é”™è¯¯: {e}")
            return None

    def judge_fix_quality(self, error_type, error_type_detail, original_code, fixed_code):
        """åˆ¤æ–­ä¿®å¤è´¨é‡"""
        try:
            prompt = self.judge_prompt_template.format(
                error_type=error_type,
                error_type_detail=error_type_detail,
                original_code=original_code,
                fixed_code=fixed_code
            )
            
            judge_response = self.get_llm_response(prompt)
            if not judge_response:
                return {
                    'judge_result': 'æ— æ•ˆåˆ¤æ–­',
                    'confidence': 0,
                    'reason': 'LLMè£åˆ¤è°ƒç”¨å¤±è´¥',
                    'raw_response': ''
                }
            
            # è§£æè£åˆ¤å“åº”
            result = self.parse_judge_response(judge_response)
            result['raw_response'] = judge_response
            return result
            
        except Exception as e:
            return {
                'judge_result': 'æ— æ•ˆåˆ¤æ–­',
                'confidence': 0,
                'reason': f'åˆ¤æ–­è¿‡ç¨‹å‡ºé”™: {str(e)}',
                'raw_response': ''
            }

    def parse_judge_response(self, response):
        """è§£æè£åˆ¤å“åº”"""
        try:
            # æå–åˆ¤æ–­ç»“æœ
            judge_pattern = r"åˆ¤æ–­ç»“æœ:\s*\[?([^\]\n]+)\]?"
            judge_match = re.search(judge_pattern, response)
            judge_result = judge_match.group(1).strip() if judge_match else "æœªçŸ¥"
            
            # æå–ç½®ä¿¡åº¦
            confidence_pattern = r"ç½®ä¿¡åº¦:\s*\[?(\d+)\]?"
            confidence_match = re.search(confidence_pattern, response)
            confidence = int(confidence_match.group(1)) if confidence_match else 0
            
            # æå–ç†ç”±
            reason_pattern = r"ç†ç”±:\s*([^\n]+(?:\n[^\n]*)*)"
            reason_match = re.search(reason_pattern, response)
            reason = reason_match.group(1).strip() if reason_match else "æœªæä¾›ç†ç”±"
            
            return {
                'judge_result': judge_result,
                'confidence': confidence,
                'reason': reason
            }
            
        except Exception as e:
            return {
                'judge_result': 'è§£æå¤±è´¥',
                'confidence': 0,
                'reason': f'å“åº”è§£æå‡ºé”™: {str(e)}'
            }

class LLMEvaluator:
    def __init__(self, base_url, headers):
        """åˆå§‹åŒ–LLMè¯„ä¼°å™¨"""
        self.base_url = base_url
        self.headers = headers
        self.fix_prompt_template = """ä½ æ˜¯ä¸€ä¸ªC++ç¼–ç¨‹ä¸“å®¶ã€‚
ç»™å®šä»¥ä¸‹åŒ…å«ç¼–è¯‘é”™è¯¯çš„C++ä»£ç ï¼š


æœ‰é—®é¢˜çš„ä»£ç ï¼š
```cpp
{error_code}
```

ç¼–è¯‘é”™è¯¯ä¿¡æ¯ï¼š
{compilation_error}

è¯·ä¿®å¤è¿™æ®µä»£ç ä¸­çš„ç¼–è¯‘é”™è¯¯ï¼Œè¦æ±‚ï¼š
1. ä¿æŒä»£ç çš„åŸå§‹æ„å›¾å’ŒåŠŸèƒ½
2. åªä¿®å¤ç¼–è¯‘é”™è¯¯ï¼Œä¸è¦æ·»åŠ ä¸å¿…è¦çš„åŠŸèƒ½
3. ç¡®ä¿ä¿®å¤åçš„ä»£ç èƒ½å¤ŸæˆåŠŸç¼–è¯‘
4. å¦‚æœéœ€è¦æ·»åŠ å¤´æ–‡ä»¶ï¼Œè¯·åŒ…å«å¿…è¦çš„å¤´æ–‡ä»¶
5. ä¿æŒä»£ç ç®€æ´æ˜äº†

è¯·ç›´æ¥è¿”å›ä¿®å¤åçš„å®Œæ•´ä»£ç ï¼Œä¸éœ€è¦å…¶ä»–è§£é‡Šã€‚"""

    def get_llm_response(self, prompt, base_url, reason=True, temperature=0.1):
        """è°ƒç”¨LLM APIè·å–å“åº”"""
        payload = {
            "model": "test", #"Qwen3-235B-A22B", #"test",
            "max_tokens": 4096,
            "temperature": temperature,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªC++ç¼–ç¨‹ä¸“å®¶ï¼Œä¸“é—¨ä¿®å¤ç¼–è¯‘é”™è¯¯ã€‚"},
                {"role": "user", "content": prompt + "/no_think" if reason else prompt}
            ],
            "stream": False
        }

        try:
            response = requests.request("POST", base_url, json=payload, headers=self.headers, verify=False)
            response.raise_for_status()
            content = json.loads(response.text)['choices'][0]['message']['content']
            return content
        except Exception as e:
            print(f"APIè°ƒç”¨é”™è¯¯: {e}")
            return None

    def generate_fix(self, error_type, error_type_detail, error_code, compilation_error):
        """ç”Ÿæˆä¿®å¤ä»£ç """
        try:
            prompt = self.fix_prompt_template.format(
                error_type=error_type,
                error_type_detail=error_type_detail,
                error_code=error_code,
                compilation_error=compilation_error
            )
            
            # è°ƒç”¨APIç”Ÿæˆä¿®å¤ä»£ç 
            generated_fix = self.get_llm_response(
                prompt, 
                base_url=self.base_url, #'http://10.55.56.14:31135/v1/chat/completions', 
                reason=False
            )
            if generated_fix:
                return generated_fix.strip()
            return ""
            
        except Exception as e:
            print(f"ç”Ÿæˆä¿®å¤ä»£ç æ—¶å‡ºé”™ ({error_type}): {str(e)}")
            return ""
        
        finally:
            # é¿å…è§¦å‘APIé€Ÿç‡é™åˆ¶
            time.sleep(0.1)

def extract_cpp_code(generated_code):
    """ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–C++ä»£ç """
    # å°è¯•æå–```cppä»£ç å—
    pattern = r"```cpp\n(.*?)```"
    match = re.search(pattern, generated_code, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # å°è¯•æå–```c++ä»£ç å—
    pattern = r"```c\+\+\n(.*?)```"
    match = re.search(pattern, generated_code, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # å°è¯•æå–```ä»£ç å—
    pattern = r"```\n(.*?)```"
    match = re.search(pattern, generated_code, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # å¦‚æœæ²¡æœ‰ä»£ç å—æ ‡è®°ï¼Œè¿”å›åŸæ–‡æœ¬
    return generated_code.strip()

def compile_cpp_code(code):
    """ä½¿ç”¨GCCç¼–è¯‘C++ä»£ç å¹¶è·å–ç¼–è¯‘ç»“æœ"""
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(suffix='.cpp', mode='w', delete=False) as temp_file:
        temp_file.write(code)
        temp_file_path = temp_file.name

    try:
        # å°è¯•ç¼–è¯‘ä»£ç 
        result = subprocess.run(
            ['g++', '-c', temp_file_path, '-std=c++11'],
            capture_output=True,
            text=True
        )
        
        # è¿”å›ç¼–è¯‘ç»“æœ
        return {
            'success': result.returncode == 0,
            'error_output': result.stderr if result.stderr else "",
            'stdout': result.stdout if result.stdout else ""
        }
    
    except Exception as e:
        return {
            'success': False,
            'error_output': f"ç¼–è¯‘è¿‡ç¨‹å‡ºé”™: {str(e)}",
            'stdout': ""
        }
    
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            remove_o_files(temp_file_path)
            # os.unlink(temp_file_path)
            # if os.path.exists(temp_file_path[:-4] + '.o'):
            #     os.unlink(temp_file_path[:-4] + '.o')
        except:
            pass

def remove_cpp_comments(code: str) -> str:
    """åˆ é™¤C++ä»£ç ä¸­çš„æ³¨é‡Š
    
    åˆ é™¤ä»¥ä¸‹ç±»å‹çš„æ³¨é‡Šï¼š
    1. å•è¡Œæ³¨é‡Š //
    2. å¤šè¡Œæ³¨é‡Š /* */
    """
    # å¤„ç†å¤šè¡Œæ³¨é‡Š
    code = re.sub(r'/\*[\s\S]*?\*/', '', code)
    
    # å¤„ç†å•è¡Œæ³¨é‡Š
    code = re.sub(r'//.*$', '', code, flags=re.MULTILINE)
    
    # åˆ é™¤ç©ºè¡Œ
    code = '\n'.join(line for line in code.splitlines() if line.strip())
    
    return code

def evaluate_single_item(evaluator, item):
    """è¯„ä¼°å•ä¸ªé”™è¯¯é¡¹ç›®"""
    try:
        error_type = item['error_type']
        error_type_detail = item['error_type_detail']
        
        # æ ¹æ®æ•°æ®æ¥æºè·å–é”™è¯¯ä»£ç å’Œç¼–è¯‘é”™è¯¯ä¿¡æ¯
        if 'error_example_wo_comments' in item:
            # ä½¿ç”¨å»æ‰æ³¨é‡Šåçš„ä»£ç 
            error_code = item['error_example_wo_comments']
            error_code_without_comments = error_code
        elif 'error_example_llm_code' in item:
            # LLMç¤ºä¾‹æ•°æ®
            error_code = item['error_example_llm_code']
            error_code_without_comments = remove_cpp_comments(error_code)
        elif 'error_example_code' in item:
            # é”™è¯¯åŸºç¡€æ•°æ®
            error_code = item['error_example_code']
            error_code_without_comments = remove_cpp_comments(error_code)
        else:
            error_code = item.get('original_code', '')
            error_code_without_comments = remove_cpp_comments(error_code)
        
        compilation_error = item.get('error_example_llm_detail', '')
        uuid = item.get('uuid', item.get('error_type', 'unknown'))
        # ç”Ÿæˆä¿®å¤ä»£ç 
        fix_response = evaluator.generate_fix(
            error_type, 
            error_type_detail, 
            error_code_without_comments, 
            compilation_error
        )
        
        if not fix_response:
            return {
                'uuid': uuid,
                'error_type': error_type,
                'success': False,
                'reason': 'LLMæœªç”Ÿæˆä¿®å¤ä»£ç ',
                'original_code': error_code,
                'fix_response': '',
                'fixed_code': '',
                'fixed_code_without_comments': '',
                'compilation_result': None
            }
        
        # æå–ä¿®å¤åçš„ä»£ç 
        fixed_code = extract_cpp_code(fix_response)
        
        if not fixed_code:
            return {
                'uuid': uuid,
                'error_type': error_type,
                'success': False,
                'reason': 'æ— æ³•ä»LLMå“åº”ä¸­æå–ä»£ç ',
                'original_code': error_code,
                'fix_response': fix_response,
                'fixed_code': '',
                'fixed_code_without_comments': '',
                'compilation_result': None
            }
        
        # å»æ‰æ³¨é‡Šåçš„ä»£ç 
        fixed_code_without_comments = remove_cpp_comments(fixed_code)
        
        # ç¼–è¯‘ä¿®å¤åçš„ä»£ç 
        compilation_result = compile_cpp_code(fixed_code)
        
        # åˆ¤æ–­æ˜¯å¦ä¿®å¤æˆåŠŸ
        success = compilation_result['success']
        reason = 'ä¿®å¤æˆåŠŸ' if success else f"ç¼–è¯‘å¤±è´¥: {compilation_result['error_output']}"
        
        return {
            'uuid': uuid,
            'error_type': error_type,
            'success': success,
            'reason': reason,
            'original_code': error_code,
            'fix_response': fix_response,
            'fixed_code': fixed_code,
            'fixed_code_without_comments': fixed_code_without_comments,
            'compilation_result': compilation_result
        }
        
    except Exception as e:
        return {
            'uuid': item.get('uuid', 'unknown'),
            'error_type': item.get('error_type', 'unknown'),
            'success': False,
            'reason': f'è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}',
            'original_code': item.get('error_example_llm_code', ''),
            'fix_response': '',
            'fixed_code': '',
            'fixed_code_without_comments': '',
            'compilation_result': None
        }

def calculate_statistics(results):
    """è®¡ç®—è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯"""
    total = len(results)
    successful = sum(1 for r in results if r['success'])
    success_rate = successful / total if total > 0 else 0
    
    # æŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡
    error_type_stats = {}
    for result in results:
        error_type = result['error_type']
        if error_type not in error_type_stats:
            error_type_stats[error_type] = {'total': 0, 'success': 0}
        
        error_type_stats[error_type]['total'] += 1
        if result['success']:
            error_type_stats[error_type]['success'] += 1
    
    # è®¡ç®—æ¯ä¸ªé”™è¯¯ç±»å‹çš„æˆåŠŸç‡
    for error_type in error_type_stats:
        stats = error_type_stats[error_type]
        stats['success_rate'] = stats['success'] / stats['total'] if stats['total'] > 0 else 0
    
    return {
        'total_items': total,
        'successful_fixes': successful,
        'overall_success_rate': success_rate,
        'error_type_stats': error_type_stats
    }

def evaluate_llm_fixes_merged(input_file, output_file, base_url, headers, max_workers=64):
    """
    è¯„ä¼°LLMä¿®å¤ç¼–è¯‘é”™è¯¯çš„èƒ½åŠ› - å¤„ç†mergedæ•°æ®æ ¼å¼
    
    è¯¥å‡½æ•°æ˜¯æ•´ä¸ªè¯„ä¼°æµç¨‹çš„ç¬¬ä¸€æ­¥ï¼Œè´Ÿè´£ï¼š
    1. åŠ è½½åŒ…å«ç¼–è¯‘é”™è¯¯çš„C++ä»£ç æ•°æ®
    2. ä½¿ç”¨LLMç”Ÿæˆä¿®å¤ä»£ç 
    3. é€šè¿‡GCCç¼–è¯‘å™¨éªŒè¯ä¿®å¤æ˜¯å¦æˆåŠŸ
    4. ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
    
    Args:
        input_file (str): è¾“å…¥çš„mergedæ ¼å¼æ•°æ®æ–‡ä»¶è·¯å¾„
                         æ–‡ä»¶åº”åŒ…å« 'llm_examples_data' å’Œ 'error_base_data' ä¸¤ä¸ªå­—æ®µ
        output_file (str): è¾“å‡ºè¯„ä¼°ç»“æœçš„JSONæ–‡ä»¶è·¯å¾„
        base_url (str): LLM APIçš„åŸºç¡€URLåœ°å€
        headers (dict): APIè¯·æ±‚å¤´ï¼Œé€šå¸¸åŒ…å«Authorizationç­‰è®¤è¯ä¿¡æ¯
        max_workers (int): å¹¶å‘å¤„ç†çš„æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤12
    
    Returns:
        dict: åŒ…å«ä»¥ä¸‹ç»“æ„çš„è¯„ä¼°ç»“æœå­—å…¸ï¼š
            {
                'evaluation_results': [
                    {
                        'uuid': 'å”¯ä¸€æ ‡è¯†ç¬¦',
                        'error_type': 'é”™è¯¯ç±»å‹',
                        'success': True/False,  # ç¼–è¯‘æ˜¯å¦æˆåŠŸ
                        'reason': 'æˆåŠŸ/å¤±è´¥åŸå› ',
                        'original_code': 'åŸå§‹é”™è¯¯ä»£ç ',
                        'fix_response': 'LLMå®Œæ•´å“åº”',
                        'fixed_code': 'æå–çš„ä¿®å¤ä»£ç ',
                        'fixed_code_without_comments': 'å»é™¤æ³¨é‡Šçš„ä¿®å¤ä»£ç ',
                        'compilation_result': 'ç¼–è¯‘è¯¦ç»†ç»“æœ'
                    },
                    ...
                ],
                'statistics': {
                    'overall': 'æ€»ä½“ç»Ÿè®¡ä¿¡æ¯',
                    'llm_examples': 'LLMç¤ºä¾‹æ•°æ®ç»Ÿè®¡',
                    'error_base': 'é”™è¯¯åŸºç¡€æ•°æ®ç»Ÿè®¡'
                },
                'metadata': {
                    'input_file': 'è¾“å…¥æ–‡ä»¶è·¯å¾„',
                    'total_evaluated': 'è¯„ä¼°æ€»æ•°',
                    'evaluation_time': 'è¯„ä¼°æ—¶é—´'
                }
            }
    
    åŠŸèƒ½æµç¨‹ï¼š
        1. æ•°æ®åŠ è½½ï¼šä»mergedæ ¼å¼æ–‡ä»¶ä¸­åŠ è½½llm_examples_dataå’Œerror_base_data
        2. å¹¶å‘å¤„ç†ï¼šä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘è°ƒç”¨LLMç”Ÿæˆä¿®å¤ä»£ç 
        3. ä»£ç æå–ï¼šä»LLMå“åº”ä¸­æå–C++ä»£ç å—
        4. ç¼–è¯‘éªŒè¯ï¼šä½¿ç”¨GCCç¼–è¯‘å™¨éªŒè¯ä¿®å¤ä»£ç æ˜¯å¦èƒ½æˆåŠŸç¼–è¯‘
        5. ç»Ÿè®¡åˆ†æï¼šè®¡ç®—æˆåŠŸç‡ã€æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„ç»Ÿè®¡ç­‰
        6. ç»“æœä¿å­˜ï¼šå°†å®Œæ•´ç»“æœä¿å­˜ä¸ºJSONæ–‡ä»¶
    
    æ³¨æ„äº‹é¡¹ï¼š
        - æ”¯æŒä¸¤ç§æ•°æ®æºï¼šllm_examples_dataï¼ˆLLMç”Ÿæˆçš„ç¤ºä¾‹ï¼‰å’Œerror_base_dataï¼ˆåŸºç¡€é”™è¯¯æ•°æ®ï¼‰
        - ä¼šè‡ªåŠ¨å»é™¤ä»£ç ä¸­çš„æ³¨é‡Šé¿å…ç¼–è¯‘å¹²æ‰°
        - ç¼–è¯‘å¤±è´¥çš„æƒ…å†µä¼šè®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
        - å‡½æ•°æ‰§è¡Œå®Œæˆåä¼šæ‰“å°è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
    """
    # è¯»å–è¾“å…¥æ•°æ®
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æå–LLMç¤ºä¾‹æ•°æ®å’Œé”™è¯¯åŸºç¡€æ•°æ®
    llm_examples_data = data.get('llm_examples_data', [])
    error_base_data = data.get('error_base_data', [])
    
    print(f"æ‰¾åˆ° {len(llm_examples_data)} ä¸ªLLMç¤ºä¾‹")
    print(f"æ‰¾åˆ° {len(error_base_data)} ä¸ªé”™è¯¯åŸºç¡€æ•°æ®")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_data = []
    
    # å¤„ç†LLMç¤ºä¾‹æ•°æ®
    for item in llm_examples_data:
        item['data_source'] = 'llm_examples'
        all_data.append(item)
    
    # å¤„ç†é”™è¯¯åŸºç¡€æ•°æ®
    for item in error_base_data:
        item['data_source'] = 'error_base'
        all_data.append(item)
    
    print(f"æ€»å…±åŠ è½½äº† {len(all_data)} ä¸ªé”™è¯¯ç¤ºä¾‹")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = LLMEvaluator(base_url, headers)
    
    # å­˜å‚¨è¯„ä¼°ç»“æœ
    results = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = []
        for item in all_data:
            future = executor.submit(evaluate_single_item, evaluator, item)
            futures.append(future)
        
        # æ”¶é›†ç»“æœ
        for future in tqdm(as_completed(futures), total=len(futures), desc="è¯„ä¼°ä¿®å¤æ•ˆæœ"):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
    
    # åˆ›å»ºæ•°æ®æºæ˜ å°„
    data_source_map = {}
    for item in all_data:
        item_uuid = item.get('uuid', item.get('error_type', 'unknown'))
        data_source_map[item_uuid] = item.get('data_source', 'unknown')
    
    # æŒ‰æ•°æ®æºåˆ†ç»„ç»Ÿè®¡
    llm_results = [r for r in results if data_source_map.get(r['uuid']) == 'llm_examples']
    error_results = [r for r in results if data_source_map.get(r['uuid']) == 'error_base']
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    overall_statistics = calculate_statistics(results)
    llm_statistics = calculate_statistics(llm_results) if llm_results else None
    error_statistics = calculate_statistics(error_results) if error_results else None
    
    # å‡†å¤‡è¾“å‡ºæ•°æ®
    output_data = {
        'evaluation_results': results,
        'statistics': {
            'overall': overall_statistics,
            'llm_examples': llm_statistics,
            'error_base': error_statistics
        },
        'metadata': {
            'input_file': input_file,
            'total_evaluated': len(results),
            'llm_examples_count': len(llm_examples_data),
            'error_base_count': len(error_base_data),
            'evaluation_time': time.strftime("%Y-%m-%d %H:%M:%S")
        }
    }
    
    # ä¿å­˜ç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nè¯„ä¼°å®Œæˆï¼")
    print(f"æ€»å…±è¯„ä¼°: {overall_statistics['total_items']} ä¸ªé¡¹ç›®")
    print(f"ä¿®å¤æˆåŠŸ: {overall_statistics['successful_fixes']} ä¸ª")
    print(f"æ€»ä½“æˆåŠŸç‡: {overall_statistics['overall_success_rate']:.2%}")
    
    if llm_statistics:
        print(f"\nLLMç¤ºä¾‹æ•°æ®ç»Ÿè®¡:")
        print(f"  è¯„ä¼°æ•°é‡: {llm_statistics['total_items']}")
        print(f"  ä¿®å¤æˆåŠŸ: {llm_statistics['successful_fixes']}")
        print(f"  æˆåŠŸç‡: {llm_statistics['overall_success_rate']:.2%}")
    
    if error_statistics:
        print(f"\né”™è¯¯åŸºç¡€æ•°æ®ç»Ÿè®¡:")
        print(f"  è¯„ä¼°æ•°é‡: {error_statistics['total_items']}")
        print(f"  ä¿®å¤æˆåŠŸ: {error_statistics['successful_fixes']}")
        print(f"  æˆåŠŸç‡: {error_statistics['overall_success_rate']:.2%}")
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # è¿”å›æ•°æ®ä»¥ä¾›åç»­å¤„ç†
    return output_data

def judge_single_item(judge, evaluation_result, original_data_map):
    """ä½¿ç”¨LLM-as-Judgeåˆ¤æ–­å•ä¸ªä¿®å¤é¡¹ç›®çš„è´¨é‡"""
    try:
        uuid = evaluation_result['uuid']
        
        # å¦‚æœç¼–è¯‘å¤±è´¥ï¼Œç›´æ¥æ ‡è®°ä¸ºæ— æ•ˆä¿®å¤
        if not evaluation_result['success']:
            return {
                'uuid': uuid,
                'error_type': evaluation_result['error_type'],
                'compile_success': False,
                'judge_result': 'æ— æ•ˆä¿®å¤',
                'confidence': 100,
                'reason': f"ä»£ç ç¼–è¯‘å¤±è´¥: {evaluation_result['reason']}",
                'original_code': evaluation_result['original_code'],
                'fixed_code': evaluation_result['fixed_code'],
                'raw_judge_response': '',
                'data_source': original_data_map.get(uuid, {}).get('data_source', 'unknown')
            }
        
        # è·å–é”™è¯¯ç±»å‹å’Œæè¿°
        original_item = original_data_map.get(uuid, {})
        error_type = evaluation_result['error_type']
        error_type_detail = original_item.get('error_type_detail', '')
        
        # ä½¿ç”¨LLMè£åˆ¤è¯„ä¼°ä¿®å¤è´¨é‡
        judge_result = judge.judge_fix_quality(
            error_type=error_type,
            error_type_detail=error_type_detail,
            original_code=evaluation_result['original_code'],
            fixed_code=evaluation_result['fixed_code']
        )
        
        return {
            'uuid': uuid,
            'error_type': error_type,
            'compile_success': True,
            'judge_result': judge_result['judge_result'],
            'confidence': judge_result['confidence'],
            'reason': judge_result['reason'],
            'original_code': evaluation_result['original_code'],
            'fixed_code': evaluation_result['fixed_code'],
            'raw_judge_response': judge_result['raw_response'],
            'data_source': original_data_map.get(uuid, {}).get('data_source', 'unknown')
        }
        
    except Exception as e:
        return {
            'uuid': evaluation_result.get('uuid', 'unknown'),
            'error_type': evaluation_result.get('error_type', 'unknown'),
            'compile_success': evaluation_result.get('success', False),
            'judge_result': 'åˆ¤æ–­å¤±è´¥',
            'confidence': 0,
            'reason': f'LLMè£åˆ¤è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}',
            'original_code': evaluation_result.get('original_code', ''),
            'fixed_code': evaluation_result.get('fixed_code', ''),
            'raw_judge_response': '',
            'data_source': 'unknown'
        }

def calculate_judge_statistics(judge_results):
    """è®¡ç®—LLMè£åˆ¤ç»Ÿè®¡ä¿¡æ¯"""
    total = len(judge_results)
    
    # æŒ‰åˆ¤æ–­ç»“æœåˆ†ç±»
    judge_categories = {}
    confidence_scores = []
    
    for result in judge_results:
        judge_result = result['judge_result']
        confidence = result['confidence']
        
        if judge_result not in judge_categories:
            judge_categories[judge_result] = 0
        judge_categories[judge_result] += 1
        
        if confidence > 0:
            confidence_scores.append(confidence)
    
    # è®¡ç®—ç½®ä¿¡åº¦ç»Ÿè®¡
    avg_confidence = statistics.mean(confidence_scores) if confidence_scores else 0
    median_confidence = statistics.median(confidence_scores) if confidence_scores else 0
    
    # æŒ‰æ•°æ®æºåˆ†ç»„ç»Ÿè®¡
    data_source_stats = {}
    for result in judge_results:
        source = result['data_source']
        if source not in data_source_stats:
            data_source_stats[source] = {
                'total': 0,
                'judge_categories': {}
            }
        
        data_source_stats[source]['total'] += 1
        judge_result = result['judge_result']
        if judge_result not in data_source_stats[source]['judge_categories']:
            data_source_stats[source]['judge_categories'][judge_result] = 0
        data_source_stats[source]['judge_categories'][judge_result] += 1
    
    # æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„ç»Ÿè®¡
    error_type_stats = {}
    for result in judge_results:
        error_type = result['error_type']
        if error_type not in error_type_stats:
            error_type_stats[error_type] = {
                'total': 0,
                'judge_categories': {}
            }
        
        error_type_stats[error_type]['total'] += 1
        judge_result = result['judge_result']
        if judge_result not in error_type_stats[error_type]['judge_categories']:
            error_type_stats[error_type]['judge_categories'][judge_result] = 0
        error_type_stats[error_type]['judge_categories'][judge_result] += 1
    
    return {
        'total_items': total,
        'judge_categories': judge_categories,
        'confidence_stats': {
            'average': avg_confidence,
            'median': median_confidence,
            'total_scored': len(confidence_scores)
        },
        'data_source_stats': data_source_stats,
        'error_type_stats': error_type_stats
    }

def Judge_compile(evaluation_data=None, evaluation_file=None, original_data_file=None, 
                 output_file=None, base_url=None, headers=None, max_workers=64):
    """
    ä½¿ç”¨LLM-as-Judgeåˆ¤æ–­ä»£ç ä¿®å¤è´¨é‡
    
    Args:
        evaluation_data: evaluate_llm_fixes_mergedå‡½æ•°è¿”å›çš„æ•°æ®ï¼Œå¦‚æœæä¾›åˆ™ç›´æ¥ä½¿ç”¨
        evaluation_file: evaluate_llm_fixes_mergedç”Ÿæˆçš„jsonæ–‡ä»¶è·¯å¾„
        original_data_file: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆmergedæ ¼å¼ï¼‰
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        base_url: LLM APIåœ°å€
        headers: APIè¯·æ±‚å¤´
        max_workers: å¹¶å‘æ•°
    """
    
    # å‚æ•°éªŒè¯
    if evaluation_data is None and evaluation_file is None:
        raise ValueError("å¿…é¡»æä¾›evaluation_dataæˆ–evaluation_fileå‚æ•°")
    
    if base_url is None or headers is None:
        raise ValueError("å¿…é¡»æä¾›base_urlå’Œheaderså‚æ•°")
    
    # åŠ è½½è¯„ä¼°æ•°æ®
    if evaluation_data is not None:
        print("ä½¿ç”¨æä¾›çš„è¯„ä¼°æ•°æ®")
        data = evaluation_data
    else:
        print(f"ä»æ–‡ä»¶åŠ è½½è¯„ä¼°æ•°æ®: {evaluation_file}")
        with open(evaluation_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    
    evaluation_results = data.get('evaluation_results', [])
    print(f"æ‰¾åˆ° {len(evaluation_results)} ä¸ªè¯„ä¼°ç»“æœ")
    
    # åŠ è½½åŸå§‹æ•°æ®ä»¥è·å–é”™è¯¯è¯¦ç»†ä¿¡æ¯
    original_data_map = {}
    if original_data_file:
        print(f"åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶: {original_data_file}")
        with open(original_data_file, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
        
        # å»ºç«‹uuidåˆ°åŸå§‹æ•°æ®çš„æ˜ å°„
        for item in original_data.get('llm_examples_data', []):
            uuid = item.get('uuid', item.get('error_type', 'unknown'))
            item['data_source'] = 'llm_examples'
            original_data_map[uuid] = item
        
        for item in original_data.get('error_base_data', []):
            uuid = item.get('uuid', item.get('error_type', 'unknown'))
            item['data_source'] = 'error_base'
            original_data_map[uuid] = item
    
    # åˆå§‹åŒ–LLMè£åˆ¤
    judge = LLMJudge(base_url, headers)
    
    # å­˜å‚¨åˆ¤æ–­ç»“æœ
    judge_results = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = []
        for result in evaluation_results:
            future = executor.submit(judge_single_item, judge, result, original_data_map)
            futures.append(future)
        
        # æ”¶é›†ç»“æœ
        for future in tqdm(as_completed(futures), total=len(futures), desc="LLMè£åˆ¤è¯„ä¼°ä¸­"):
            try:
                result = future.result()
                judge_results.append(result)
            except Exception as e:
                print(f"å¤„ç†LLMè£åˆ¤ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    judge_statistics = calculate_judge_statistics(judge_results)
    
    # å‡†å¤‡è¾“å‡ºæ•°æ®
    output_data = {
        'judge_results': judge_results,
        'judge_statistics': judge_statistics,
        'original_evaluation_statistics': data.get('statistics', {}),
        'metadata': {
            'evaluation_file': evaluation_file,
            'original_data_file': original_data_file,
            'total_judged': len(judge_results),
            'judge_time': time.strftime("%Y-%m-%d %H:%M:%S"),
            'api_config': {
                'base_url': base_url,
                'max_workers': max_workers
            }
        }
    }
    
    # ä¿å­˜ç»“æœ
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        print(f"LLMè£åˆ¤ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nLLMè£åˆ¤è¯„ä¼°å®Œæˆï¼")
    print(f"æ€»å…±è¯„ä¼°: {judge_statistics['total_items']} ä¸ªé¡¹ç›®")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {judge_statistics['confidence_stats']['average']:.1f}")
    print(f"ä¸­ä½æ•°ç½®ä¿¡åº¦: {judge_statistics['confidence_stats']['median']:.1f}")
    
    print(f"\nåˆ¤æ–­ç»“æœåˆ†å¸ƒ:")
    for category, count in judge_statistics['judge_categories'].items():
        percentage = count / judge_statistics['total_items'] * 100
        print(f"  {category}: {count} ({percentage:.1f}%)")
    
    if judge_statistics['data_source_stats']:
        print(f"\næŒ‰æ•°æ®æºç»Ÿè®¡:")
        for source, stats in judge_statistics['data_source_stats'].items():
            print(f"  {source}: {stats['total']} ä¸ªé¡¹ç›®")
            for category, count in stats['judge_categories'].items():
                percentage = count / stats['total'] * 100
                print(f"    {category}: {count} ({percentage:.1f}%)")
    
    print(f"\næŒ‰é”™è¯¯ç±»å‹ç»Ÿè®¡:")
    for error_type, stats in judge_statistics['error_type_stats'].items():
        print(f"  {error_type}: {stats['total']} ä¸ªé¡¹ç›®")
        # åªæ˜¾ç¤ºçœŸæ­£ä¿®å¤çš„æ¯”ä¾‹
        true_fix_count = stats['judge_categories'].get('çœŸæ­£ä¿®å¤', 0)
        percentage = true_fix_count / stats['total'] * 100
        print(f"    çœŸæ­£ä¿®å¤: {true_fix_count} ({percentage:.1f}%)")
    
    return output_data

def evaluate_llm_fixes(input_file, output_file, base_url, headers, max_workers=64):
    """è¯„ä¼°LLMä¿®å¤ç¼–è¯‘é”™è¯¯çš„èƒ½åŠ› - åŸå§‹æ ¼å¼"""
    # è¯»å–è¾“å…¥æ•°æ®
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # data = data["failed_items"]
    print(f"åŠ è½½äº† {len(data)} ä¸ªé”™è¯¯ç¤ºä¾‹")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = LLMEvaluator(base_url, headers)
    
    # å­˜å‚¨è¯„ä¼°ç»“æœ
    results = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = []
        for item in data:
            future = executor.submit(evaluate_single_item, evaluator, item)
            futures.append(future)
        
        # æ”¶é›†ç»“æœ
        for future in tqdm(as_completed(futures), total=len(futures), desc="è¯„ä¼°ä¿®å¤æ•ˆæœ"):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    statistics_info = calculate_statistics(results)
    
    # å‡†å¤‡è¾“å‡ºæ•°æ®
    output_data = {
        'evaluation_results': results,
        'statistics': statistics_info,
        'metadata': {
            'input_file': input_file,
            'total_evaluated': len(results),
            'evaluation_time': time.strftime("%Y-%m-%d %H:%M:%S")
        }
    }
    
    # ä¿å­˜ç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nè¯„ä¼°å®Œæˆï¼")
    print(f"æ€»å…±è¯„ä¼°: {statistics_info['total_items']} ä¸ªé¡¹ç›®")
    print(f"ä¿®å¤æˆåŠŸ: {statistics_info['successful_fixes']} ä¸ª")
    print(f"æ€»ä½“æˆåŠŸç‡: {statistics_info['overall_success_rate']:.2%}")
    print(f"\nå„é”™è¯¯ç±»å‹æˆåŠŸç‡:")
    for error_type, stats in statistics_info['error_type_stats'].items():
        print(f"  {error_type}: {stats['success']}/{stats['total']} ({stats['success_rate']:.2%})")
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def main():
    """
    ä¸»å‡½æ•° - å®Œæ•´çš„LLMä»£ç ä¿®å¤åŒé‡è¯„ä¼°æµç¨‹
    
    è¯¥å‡½æ•°æ‰§è¡Œå®Œæ•´çš„ä¸¤é˜¶æ®µè¯„ä¼°ï¼š
    1. ç¬¬ä¸€é˜¶æ®µï¼šç¼–è¯‘å™¨è¯„ä¼° - ä½¿ç”¨GCCéªŒè¯LLMä¿®å¤çš„ä»£ç æ˜¯å¦èƒ½ç¼–è¯‘é€šè¿‡
    2. ç¬¬äºŒé˜¶æ®µï¼šLLMè£åˆ¤è¯„ä¼° - ä½¿ç”¨LLM-as-Judgeåˆ¤æ–­ä¿®å¤è´¨é‡æ˜¯å¦çœŸæ­£è§£å†³é—®é¢˜
    
    å®Œæ•´æµç¨‹è¯´æ˜ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                        æ•°æ®è¾“å…¥é˜¶æ®µ                              â”‚
    â”‚ 1. åŠ è½½mergedæ ¼å¼çš„ç¼–è¯‘é”™è¯¯æ•°æ®é›†                                 â”‚
    â”‚    - llm_examples_data: LLMç”Ÿæˆçš„é”™è¯¯ç¤ºä¾‹                        â”‚
    â”‚    - error_base_data: åŸºç¡€é”™è¯¯æ•°æ®                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    ç¬¬ä¸€é˜¶æ®µï¼šç¼–è¯‘å™¨è¯„ä¼°                          â”‚
    â”‚ 1. ä½¿ç”¨LLMç”Ÿæˆä¿®å¤ä»£ç                                            â”‚
    â”‚ 2. æå–C++ä»£ç å—                                                â”‚
    â”‚ 3. ä½¿ç”¨GCCç¼–è¯‘å™¨éªŒè¯                                             â”‚
    â”‚ 4. è®°å½•ç¼–è¯‘æˆåŠŸ/å¤±è´¥                                             â”‚
    â”‚ 5. ç”Ÿæˆç¼–è¯‘å™¨è¯„ä¼°æŠ¥å‘Š                                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   ç¬¬äºŒé˜¶æ®µï¼šLLMè£åˆ¤è¯„ä¼°                          â”‚
    â”‚ 1. åŸºäºç¼–è¯‘æˆåŠŸçš„ä¿®å¤ä»£ç                                         â”‚
    â”‚ 2. ä½¿ç”¨LLM-as-Judgeåˆ†æä¿®å¤è´¨é‡                                 â”‚
    â”‚ 3. åˆ¤æ–­æ˜¯å¦çœŸæ­£ä¿®å¤ï¼ˆvsç®€å•åˆ é™¤ï¼‰                               â”‚
    â”‚ 4. æä¾›ç½®ä¿¡åº¦è¯„åˆ†                                               â”‚
    â”‚ 5. ç”Ÿæˆæœ€ç»ˆè´¨é‡è¯„ä¼°æŠ¥å‘Š                                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                        ç»“æœè¾“å‡º                                 â”‚
    â”‚ - ç¼–è¯‘å™¨è¯„ä¼°ç»“æœæ–‡ä»¶                                             â”‚
    â”‚ - LLMè£åˆ¤è¯„ä¼°ç»“æœæ–‡ä»¶                                           â”‚
    â”‚ - è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯æ‰“å°                                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    print("ğŸš€ å¼€å§‹LLMä»£ç ä¿®å¤åŒé‡è¯„ä¼°æµç¨‹")
    print("=" * 80)
    
    # ===================== é…ç½®å‚æ•° =====================
    # æ•°æ®æ–‡ä»¶é…ç½®
    input_file = '/mnt/tenant-home_speed/zhaijucai/evaluate/merged_gcc_compatible_data.json'  # è¾“å…¥çš„é”™è¯¯æ•°æ®é›†
    output_file = 'llm_fix_evaluation_results_merged_data_test_32B.json'  # ç¼–è¯‘å™¨è¯„ä¼°ç»“æœ
    judge_output_file = 'llm_judge_evaluation_results_32B.json'  # LLMè£åˆ¤è¯„ä¼°ç»“æœ
    
    # APIé…ç½® - ä»£ç ä¿®å¤æ¨¡å‹
    fix_model_base_url = 'http://localhost:7801/v1/chat/completions'
    # APIé…ç½® - è£åˆ¤æ¨¡å‹  
    judge_model_base_url = 'http://localhost:7804/v1/chat/completions'
    
    headers = {
        "Authorization": "TEST-46542881-54d4-4096-b93d-6d5a3db326ac",
        "Content-Type": "application/json"
    }
    
    # ===================== è¾“å…¥éªŒè¯ =====================
    if not os.path.exists(input_file):
        print(f"âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ {input_file} ä¸å­˜åœ¨")
        print("è¯·ç¡®ä¿å·²è¿è¡Œfilter_data_to_remain_gcc_compile_error.pyç”Ÿæˆmergedæ•°æ®")
        return
    
    print(f"ğŸ“‚ è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"ğŸ“„ ç¼–è¯‘è¯„ä¼°è¾“å‡º: {output_file}")
    print(f"ğŸ“„ è£åˆ¤è¯„ä¼°è¾“å‡º: {judge_output_file}")
    print(f"ğŸ”§ ä¿®å¤æ¨¡å‹API: {fix_model_base_url}")
    print(f"âš–ï¸  è£åˆ¤æ¨¡å‹API: {judge_model_base_url}")
    
    # ===================== ç¬¬ä¸€é˜¶æ®µï¼šç¼–è¯‘å™¨è¯„ä¼° =====================
    print("\n" + "=" * 60)
    print("ğŸ“ ç¬¬ä¸€é˜¶æ®µ: LLMä¿®å¤ä»£ç  + GCCç¼–è¯‘å™¨è¯„ä¼°")
    print("=" * 60)
    print("åŠŸèƒ½ï¼š")
    print("  1. ä½¿ç”¨LLMç”Ÿæˆä¿®å¤ä»£ç ")
    print("  2. æå–C++ä»£ç å—")
    print("  3. ä½¿ç”¨GCCç¼–è¯‘å™¨éªŒè¯ä¿®å¤æ•ˆæœ")
    print("  4. ç»Ÿè®¡ç¼–è¯‘æˆåŠŸç‡")
    print()
    
    try:
        evaluation_data = evaluate_llm_fixes_merged(
            input_file=input_file,
            output_file=output_file,
            base_url=fix_model_base_url,
            headers=headers,
            max_workers=64  # å¹¶å‘æ•°
        )
        print("âœ… ç¬¬ä¸€é˜¶æ®µè¯„ä¼°å®Œæˆ")
    except Exception as e:
        print(f"âŒ ç¬¬ä¸€é˜¶æ®µè¯„ä¼°å¤±è´¥: {str(e)}")
        return
    
    # ===================== ç¬¬äºŒé˜¶æ®µï¼šLLMè£åˆ¤è¯„ä¼° =====================
    print("\n" + "=" * 60)
    print("âš–ï¸  ç¬¬äºŒé˜¶æ®µ: LLM-as-Judge ä¿®å¤è´¨é‡è¯„ä¼°")
    print("=" * 60)
    print("åŠŸèƒ½ï¼š")
    print("  1. åŸºäºç¼–è¯‘æˆåŠŸçš„ä»£ç è¿›è¡Œè´¨é‡åˆ¤æ–­")
    print("  2. åŒºåˆ†'çœŸæ­£ä¿®å¤' vs 'ç®€å•åˆ é™¤'")
    print("  3. æä¾›ç½®ä¿¡åº¦è¯„åˆ†å’Œè¯¦ç»†ç†ç”±")
    print("  4. æŒ‰é”™è¯¯ç±»å‹å’Œæ•°æ®æºç»Ÿè®¡")
    print()
    
    try:
        print("ğŸ”„ ä½¿ç”¨æ–¹å¼1: ç›´æ¥ä½¿ç”¨ç¬¬ä¸€é˜¶æ®µè¿”å›çš„æ•°æ®")
        judge_data = Judge_compile(
            evaluation_data=evaluation_data,  # ç›´æ¥ä½¿ç”¨è¿”å›çš„æ•°æ®
            original_data_file=input_file,    # åŸå§‹æ•°æ®æ–‡ä»¶ç”¨äºè·å–é”™è¯¯è¯¦ç»†ä¿¡æ¯
            output_file=judge_output_file,
            base_url=judge_model_base_url,
            headers=headers,
            max_workers=64
        )
        print("âœ… ç¬¬äºŒé˜¶æ®µè¯„ä¼°å®Œæˆ")
    except Exception as e:
        print(f"âŒ ç¬¬äºŒé˜¶æ®µè¯„ä¼°å¤±è´¥: {str(e)}")
        return
    
    # ===================== æ€»ç»“æŠ¥å‘Š =====================
    print("\n" + "=" * 60)
    print("ğŸ“Š åŒé‡è¯„ä¼°æµç¨‹å®Œæˆæ€»ç»“")
    print("=" * 60)
    
    if evaluation_data and 'statistics' in evaluation_data:
        overall_stats = evaluation_data['statistics']['overall']
        print(f"ğŸ”§ ç¼–è¯‘å™¨è¯„ä¼°ç»“æœ:")
        print(f"   æ€»æ ·æœ¬æ•°: {overall_stats['total_items']}")
        print(f"   ç¼–è¯‘æˆåŠŸ: {overall_stats['successful_fixes']}")
        print(f"   ç¼–è¯‘æˆåŠŸç‡: {overall_stats['overall_success_rate']:.2%}")
    
    if judge_data and 'judge_statistics' in judge_data:
        judge_stats = judge_data['judge_statistics']
        print(f"\nâš–ï¸  LLMè£åˆ¤è¯„ä¼°ç»“æœ:")
        print(f"   è¯„ä¼°æ ·æœ¬æ•°: {judge_stats['total_items']}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {judge_stats['confidence_stats']['average']:.1f}")
        if 'çœŸæ­£ä¿®å¤' in judge_stats['judge_categories']:
            true_fix_count = judge_stats['judge_categories']['çœŸæ­£ä¿®å¤']
            true_fix_rate = true_fix_count / judge_stats['total_items'] * 100
            print(f"   çœŸæ­£ä¿®å¤æ•°é‡: {true_fix_count} ({true_fix_rate:.1f}%)")
    
    print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
    print(f"   ç¼–è¯‘å™¨è¯„ä¼°: {output_file}")
    print(f"   è£åˆ¤è¯„ä¼°: {judge_output_file}")
    print("\nğŸ‰ åŒé‡è¯„ä¼°æµç¨‹å…¨éƒ¨å®Œæˆï¼")

def remove_o_files(file_path):
    try:
        # è·å–å½“å‰ç»ˆç«¯æ‰€åœ¨ç›®å½•
        dir_path = os.getcwd()
        # print(f"åˆ é™¤.oæ–‡ä»¶æ‰€åœ¨ç›®å½•: {dir_path}")
        # éå†ç›®å½•ä¸­æ‰€æœ‰æ–‡ä»¶
        for filename in os.listdir(dir_path):
            if filename.endswith('.o'):
                file_to_remove = os.path.join(dir_path, filename)
                os.unlink(file_to_remove)
        return True
    except Exception as e:
        print(f"åˆ é™¤.oæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return False 
    
def demo_judge_from_file():
    """
    æ¼”ç¤ºä»æ–‡ä»¶åŠ è½½æ•°æ®è¿›è¡ŒLLMè£åˆ¤è¯„ä¼°
    
    è¯¥å‡½æ•°æ¼”ç¤ºå¦‚ä½•å•ç‹¬è¿è¡ŒLLM-as-Judgeè¯„ä¼°ï¼Œé€‚ç”¨äºä»¥ä¸‹åœºæ™¯ï¼š
    1. å·²ç»æœ‰äº†ç¼–è¯‘å™¨è¯„ä¼°ç»“æœæ–‡ä»¶
    2. åªæƒ³é‡æ–°è¿è¡ŒLLMè£åˆ¤è¯„ä¼°éƒ¨åˆ†
    3. ä½¿ç”¨ä¸åŒçš„è£åˆ¤æ¨¡å‹é‡æ–°è¯„ä¼°
    4. è°ƒè¯•å’Œæµ‹è¯•LLMè£åˆ¤åŠŸèƒ½
    
    ä½¿ç”¨åœºæ™¯ï¼š
    - å½“ç¬¬ä¸€é˜¶æ®µï¼ˆç¼–è¯‘å™¨è¯„ä¼°ï¼‰å·²ç»å®Œæˆï¼Œåªéœ€è¦è¿›è¡Œè´¨é‡åˆ¤æ–­
    - å½“éœ€è¦ä½¿ç”¨ä¸åŒå‚æ•°é‡æ–°è¿è¡Œè£åˆ¤è¯„ä¼°æ—¶
    - å½“éœ€è¦å¯¹å†å²è¯„ä¼°ç»“æœè¿›è¡Œé‡æ–°åˆ†ææ—¶
    """
    print("\n" + "ğŸ”„ æ¼”ç¤º: ä»æ–‡ä»¶åŠ è½½æ•°æ®è¿›è¡ŒLLM-as-Judgeè¯„ä¼°")
    print("=" * 70)
    print("ğŸ“‹ ä½¿ç”¨åœºæ™¯:")
    print("  1. å·²æœ‰ç¼–è¯‘å™¨è¯„ä¼°ç»“æœï¼Œåªéœ€è´¨é‡åˆ¤æ–­")
    print("  2. é‡æ–°ä½¿ç”¨ä¸åŒè£åˆ¤æ¨¡å‹è¯„ä¼°")
    print("  3. è°ƒè¯•å’Œæµ‹è¯•LLMè£åˆ¤åŠŸèƒ½")
    print("=" * 70)
    
    # é…ç½®å‚æ•°
    evaluation_file = '/home/10350334@zte.intra/Desktop/ç ”å‘ææ•ˆä¸‰é˜¶æ®µ/medical_cpt/error_data/llm_fix_evaluation_results_merged_data_1.5b_rl_lm_228.json'  # ä¹‹å‰ç”Ÿæˆçš„è¯„ä¼°æ–‡ä»¶
    original_data_file = 'data_final/merged_gcc_compatible_data.json'
    judge_output_file = 'llm_judge_from_file_results.json'
    
    # APIé…ç½®
    base_url = 'http://10.55.42.83:31032/v1/chat/completions'
    headers = {
        "Authorization": "TEST-46542881-54d4-4096-b93d-6d5a3db326ac",
        "Content-Type": "application/json"
    }
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(evaluation_file):
        print(f"é”™è¯¯: è¯„ä¼°æ–‡ä»¶ {evaluation_file} ä¸å­˜åœ¨")
        print("è¯·å…ˆè¿è¡Œmain()ç”Ÿæˆè¯„ä¼°ç»“æœæ–‡ä»¶")
        return
    
    if not os.path.exists(original_data_file):
        print(f"é”™è¯¯: åŸå§‹æ•°æ®æ–‡ä»¶ {original_data_file} ä¸å­˜åœ¨")
        return
    
    # æ–¹å¼2: ä»æ–‡ä»¶åŠ è½½æ•°æ®
    print("ä½¿ç”¨æ–¹å¼2: ä»æ–‡ä»¶åŠ è½½è¯„ä¼°æ•°æ®")
    Judge_compile(
        evaluation_file=evaluation_file,     # ä»æ–‡ä»¶åŠ è½½è¯„ä¼°æ•°æ®
        original_data_file=original_data_file,  # åŸå§‹æ•°æ®æ–‡ä»¶
        output_file=judge_output_file,
        base_url=base_url,
        headers=headers,
        max_workers=64
    )
def data_analyze(judge_result_file, output_file=None, show_details=True):
    '''
    åˆ†æJudge_compileå‡½æ•°è¾“å‡ºçš„æ•°æ®ï¼Œç»Ÿè®¡compile_successã€judge_resultã€data_sourceçš„åˆ†å¸ƒæƒ…å†µ
    
    Args:
        judge_result_file (str): Judge_compileå‡½æ•°ç”Ÿæˆçš„ç»“æœæ–‡ä»¶è·¯å¾„
        output_file (str, optional): ç»Ÿè®¡ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æä¾›åˆ™åªæ‰“å°
        show_details (bool): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        
    Returns:
        dict: åŒ…å«æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    '''
    
    print("ğŸ“Š å¼€å§‹æ•°æ®åˆ†æå’Œç»Ÿè®¡")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    try:
        with open(judge_result_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        judge_results = data.get('judge_results', [])
        print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {judge_result_file}")
        print(f"ğŸ“ˆ æ€»æ ·æœ¬æ•°é‡: {len(judge_results)}")
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
        return None
    
    if not judge_results:
        print("âš ï¸ æ•°æ®æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°judge_results")
        return None
    
    # åˆå§‹åŒ–ç»Ÿè®¡è®¡æ•°å™¨
    compile_success_stats = {"æˆåŠŸ": 0, "å¤±è´¥": 0}
    judge_result_stats = {}
    data_source_stats = {}
    error_type_stats = {}
    confidence_scores = []
    
    # äº¤å‰ç»Ÿè®¡ - æŒ‰æ•°æ®æºåˆ†ç»„çš„ç¼–è¯‘æˆåŠŸæƒ…å†µ
    cross_compile_by_source = {}
    # äº¤å‰ç»Ÿè®¡ - æŒ‰æ•°æ®æºåˆ†ç»„çš„è£åˆ¤ç»“æœ
    cross_judge_by_source = {}
    # äº¤å‰ç»Ÿè®¡ - æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„çš„è£åˆ¤ç»“æœ
    cross_judge_by_error_type = {}
    
    print("\nğŸ” æ•°æ®åˆ†æä¸­...")
    
    # éå†æ‰€æœ‰ç»“æœè¿›è¡Œç»Ÿè®¡
    for result in judge_results:
        compile_success = result.get('compile_success', False)
        judge_result = result.get('judge_result', 'æœªçŸ¥').strip('*').strip()  # å»é™¤å¯èƒ½çš„**æ ‡è®°
        data_source = result.get('data_source', 'æœªçŸ¥')
        error_type = result.get('error_type', 'æœªçŸ¥')
        confidence = result.get('confidence', 0)
        
        # ç¼–è¯‘æˆåŠŸç»Ÿè®¡
        if compile_success:
            compile_success_stats["æˆåŠŸ"] += 1
        else:
            compile_success_stats["å¤±è´¥"] += 1
        
        # è£åˆ¤ç»“æœç»Ÿè®¡
        if judge_result not in judge_result_stats:
            judge_result_stats[judge_result] = 0
        judge_result_stats[judge_result] += 1
        
        # æ•°æ®æºç»Ÿè®¡
        if data_source not in data_source_stats:
            data_source_stats[data_source] = 0
        data_source_stats[data_source] += 1
        
        # é”™è¯¯ç±»å‹ç»Ÿè®¡
        if error_type not in error_type_stats:
            error_type_stats[error_type] = 0
        error_type_stats[error_type] += 1
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        if confidence > 0:
            confidence_scores.append(confidence)
        
        # äº¤å‰ç»Ÿè®¡ - æŒ‰æ•°æ®æºåˆ†ç»„çš„ç¼–è¯‘æˆåŠŸæƒ…å†µ
        if data_source not in cross_compile_by_source:
            cross_compile_by_source[data_source] = {"æˆåŠŸ": 0, "å¤±è´¥": 0, "æ€»æ•°": 0}
        
        cross_compile_by_source[data_source]["æ€»æ•°"] += 1
        if compile_success:
            cross_compile_by_source[data_source]["æˆåŠŸ"] += 1
        else:
            cross_compile_by_source[data_source]["å¤±è´¥"] += 1
        
        # äº¤å‰ç»Ÿè®¡ - æŒ‰æ•°æ®æºåˆ†ç»„çš„è£åˆ¤ç»“æœ
        if data_source not in cross_judge_by_source:
            cross_judge_by_source[data_source] = {}
        if judge_result not in cross_judge_by_source[data_source]:
            cross_judge_by_source[data_source][judge_result] = 0
        cross_judge_by_source[data_source][judge_result] += 1
        
        # äº¤å‰ç»Ÿè®¡ - æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„çš„è£åˆ¤ç»“æœ
        if error_type not in cross_judge_by_error_type:
            cross_judge_by_error_type[error_type] = {}
        if judge_result not in cross_judge_by_error_type[error_type]:
            cross_judge_by_error_type[error_type][judge_result] = 0
        cross_judge_by_error_type[error_type][judge_result] += 1
    
    # è®¡ç®—ç½®ä¿¡åº¦ç»Ÿè®¡
    confidence_stats = {}
    if confidence_scores:
        confidence_stats = {
            "å¹³å‡å€¼": statistics.mean(confidence_scores),
            "ä¸­ä½æ•°": statistics.median(confidence_scores),
            "æœ€å¤§å€¼": max(confidence_scores),
            "æœ€å°å€¼": min(confidence_scores),
            "æ ·æœ¬æ•°": len(confidence_scores)
        }
    
    # æ±‡æ€»ç»Ÿè®¡ç»“æœ
    analysis_results = {
        "åŸºæœ¬ç»Ÿè®¡": {
            "æ€»æ ·æœ¬æ•°": len(judge_results),
            "ç¼–è¯‘æˆåŠŸç»Ÿè®¡": compile_success_stats,
            "è£åˆ¤ç»“æœç»Ÿè®¡": judge_result_stats,
            "æ•°æ®æºç»Ÿè®¡": data_source_stats,
            "é”™è¯¯ç±»å‹ç»Ÿè®¡": error_type_stats,
            "ç½®ä¿¡åº¦ç»Ÿè®¡": confidence_stats
        },
        "äº¤å‰ç»Ÿè®¡": {
            "æŒ‰æ•°æ®æºçš„ç¼–è¯‘æˆåŠŸæƒ…å†µ": cross_compile_by_source,
            "æŒ‰æ•°æ®æºçš„è£åˆ¤ç»“æœ": cross_judge_by_source,
            "æŒ‰é”™è¯¯ç±»å‹çš„è£åˆ¤ç»“æœ": cross_judge_by_error_type
        },
        "åˆ†æå…ƒæ•°æ®": {
            "åˆ†ææ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S"),
            "æºæ–‡ä»¶": judge_result_file
        }
    }
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    total_samples = len(judge_results)
    
    print("\n" + "=" * 60)
    print("ğŸ“Š åŸºæœ¬ç»Ÿè®¡ç»“æœ")
    print("=" * 60)
    
    # ç¼–è¯‘æˆåŠŸç»Ÿè®¡
    print(f"\nğŸ”§ ç¼–è¯‘æˆåŠŸæƒ…å†µ:")
    for status, count in compile_success_stats.items():
        percentage = count / total_samples * 100
        print(f"   {status}: {count} ({percentage:.1f}%)")
    
    # è£åˆ¤ç»“æœç»Ÿè®¡
    print(f"\nâš–ï¸ LLMè£åˆ¤ç»“æœåˆ†å¸ƒ:")
    for result, count in sorted(judge_result_stats.items(), key=lambda x: x[1], reverse=True):
        percentage = count / total_samples * 100
        print(f"   {result}: {count} ({percentage:.1f}%)")
    
    # æ•°æ®æºç»Ÿè®¡
    print(f"\nğŸ“‚ æ•°æ®æºåˆ†å¸ƒ:")
    for source, count in sorted(data_source_stats.items(), key=lambda x: x[1], reverse=True):
        percentage = count / total_samples * 100
        print(f"   {source}: {count} ({percentage:.1f}%)")
    
    # ç½®ä¿¡åº¦ç»Ÿè®¡
    if confidence_stats:
        print(f"\nğŸ¯ ç½®ä¿¡åº¦ç»Ÿè®¡:")
        print(f"   å¹³å‡å€¼: {confidence_stats['å¹³å‡å€¼']:.1f}")
        print(f"   ä¸­ä½æ•°: {confidence_stats['ä¸­ä½æ•°']:.1f}")
        print(f"   èŒƒå›´: {confidence_stats['æœ€å°å€¼']:.0f} - {confidence_stats['æœ€å¤§å€¼']:.0f}")
        print(f"   æœ‰æ•ˆæ ·æœ¬: {confidence_stats['æ ·æœ¬æ•°']}")
    
    if show_details:
        print("\n" + "=" * 60)
        print("ğŸ“ˆ äº¤å‰ç»Ÿè®¡åˆ†æ")
        print("=" * 60)
        
        # æŒ‰æ•°æ®æºçš„ç¼–è¯‘æˆåŠŸæƒ…å†µ
        print(f"\nğŸ”§ æŒ‰æ•°æ®æºåˆ†ç»„çš„ç¼–è¯‘æˆåŠŸæƒ…å†µ:")
        for source, stats in cross_compile_by_source.items():
            success_rate = stats["æˆåŠŸ"] / stats["æ€»æ•°"] * 100 if stats["æ€»æ•°"] > 0 else 0
            print(f"   {source}:")
            print(f"     æ€»æ•°: {stats['æ€»æ•°']}")
            print(f"     æˆåŠŸ: {stats['æˆåŠŸ']} ({success_rate:.1f}%)")
            print(f"     å¤±è´¥: {stats['å¤±è´¥']} ({100-success_rate:.1f}%)")
        
        # æŒ‰æ•°æ®æºçš„è£åˆ¤ç»“æœ
        print(f"\nâš–ï¸ æŒ‰æ•°æ®æºåˆ†ç»„çš„è£åˆ¤ç»“æœ:")
        for source, results in cross_judge_by_source.items():
            total_for_source = sum(results.values())
            print(f"   {source} (æ€»æ•°: {total_for_source}):")
            for result, count in sorted(results.items(), key=lambda x: x[1], reverse=True):
                percentage = count / total_for_source * 100
                print(f"     {result}: {count} ({percentage:.1f}%)")
        
        # é‡ç‚¹é”™è¯¯ç±»å‹åˆ†æï¼ˆæ˜¾ç¤ºå‰10ä¸ªï¼‰
        print(f"\nğŸ› ä¸»è¦é”™è¯¯ç±»å‹çš„è£åˆ¤ç»“æœ (Top 10):")
        sorted_error_types = sorted(error_type_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        for error_type, total_count in sorted_error_types:
            if error_type in cross_judge_by_error_type:
                results = cross_judge_by_error_type[error_type]
                true_fix_count = results.get('çœŸæ­£ä¿®å¤', 0)
                true_fix_rate = true_fix_count / total_count * 100
                print(f"   {error_type} (æ€»æ•°: {total_count}):")
                print(f"     çœŸæ­£ä¿®å¤: {true_fix_count} ({true_fix_rate:.1f}%)")
                other_results = {k: v for k, v in results.items() if k != 'çœŸæ­£ä¿®å¤'}
                if other_results:
                    for result, count in sorted(other_results.items(), key=lambda x: x[1], reverse=True):
                        percentage = count / total_count * 100
                        print(f"     {result}: {count} ({percentage:.1f}%)")
    
    # ä¿å­˜ç»“æœ
    if output_file:
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜ç»Ÿè®¡ç»“æœå¤±è´¥: {str(e)}")
    
    print(f"\nâœ… æ•°æ®åˆ†æå®Œæˆï¼")
    
    return analysis_results

def demo_data_analyze():
    """
    æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨data_analyzeå‡½æ•°è¿›è¡Œæ•°æ®åˆ†æ
    """
    print("ğŸ“Š æ¼”ç¤ºæ•°æ®åˆ†æåŠŸèƒ½")
    print("=" * 50)
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    judge_result_file = 'llm_judge_evaluation_results_32B.json'  # Judge_compileçš„è¾“å‡ºæ–‡ä»¶
    analysis_output_file = 'data_analysis_results.json'    # åˆ†æç»“æœä¿å­˜æ–‡ä»¶
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(judge_result_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {judge_result_file}")
        print("è¯·å…ˆè¿è¡ŒJudge_compileå‡½æ•°ç”Ÿæˆç»“æœæ–‡ä»¶")
        return
    
    # æ‰§è¡Œæ•°æ®åˆ†æ
    print(f"ğŸ” åˆ†ææ–‡ä»¶: {judge_result_file}")
    
    try:
        # è°ƒç”¨æ•°æ®åˆ†æå‡½æ•°
        results = data_analyze(
            judge_result_file=judge_result_file,
            output_file=analysis_output_file,
            show_details=True  # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
        )
        
        if results:
            print("\nğŸ‰ æ•°æ®åˆ†æå®Œæˆï¼")
            print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {analysis_output_file}")
            
            # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡æ‘˜è¦
            basic_stats = results.get('åŸºæœ¬ç»Ÿè®¡', {})
            print(f"\nğŸ“‹ å…³é”®æŒ‡æ ‡æ‘˜è¦:")
            print(f"   æ€»æ ·æœ¬æ•°: {basic_stats.get('æ€»æ ·æœ¬æ•°', 0)}")
            
            compile_stats = basic_stats.get('ç¼–è¯‘æˆåŠŸç»Ÿè®¡', {})
            if compile_stats:
                total = sum(compile_stats.values())
                success_rate = compile_stats.get('æˆåŠŸ', 0) / total * 100 if total > 0 else 0
                print(f"   ç¼–è¯‘æˆåŠŸç‡: {success_rate:.1f}%")
            
            judge_stats = basic_stats.get('è£åˆ¤ç»“æœç»Ÿè®¡', {})
            if judge_stats:
                true_fix_count = judge_stats.get('çœŸæ­£ä¿®å¤', 0)
                total_judge = sum(judge_stats.values())
                true_fix_rate = true_fix_count / total_judge * 100 if total_judge > 0 else 0
                print(f"   çœŸæ­£ä¿®å¤ç‡: {true_fix_rate:.1f}%")
                
        else:
            print("âŒ æ•°æ®åˆ†æå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")

if __name__ == "__main__":
    # å¯ä»¥é€‰æ‹©è¿è¡Œä¸åŒçš„å‡½æ•°
    
    # é€‰é¡¹1: è¿è¡Œå®Œæ•´çš„åŒé‡è¯„ä¼°æµç¨‹
    main()
    
    # é€‰é¡¹2: è¿è¡ŒLLMè£åˆ¤è¯„ä¼°ï¼ˆä»æ–‡ä»¶åŠ è½½ï¼‰
    # demo_judge_from_file()
    
    # é€‰é¡¹3: è¿è¡Œæ•°æ®åˆ†æ
    demo_data_analyze()
    
    # å¦‚æœè¦è¿è¡ŒåŸæ¥çš„demo_judge_from_fileï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # demo_judge_from_file()

"""
===============================================================================
                        LLMä»£ç ä¿®å¤åŒé‡è¯„ä¼°ç³»ç»Ÿä½¿ç”¨è¯´æ˜
===============================================================================

ğŸ¯ ç³»ç»Ÿç›®æ ‡ï¼š
   è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¿®å¤C++ç¼–è¯‘é”™è¯¯çš„èƒ½åŠ›ï¼Œä¸ä»…éªŒè¯ä»£ç èƒ½å¦ç¼–è¯‘é€šè¿‡ï¼Œ
   æ›´é‡è¦çš„æ˜¯åˆ¤æ–­ä¿®å¤æ˜¯å¦çœŸæ­£è§£å†³é—®é¢˜ï¼Œè€Œéç®€å•åˆ é™¤å‡ºé”™ä»£ç ã€‚

ğŸ“‹ ç³»ç»Ÿæ¶æ„ï¼š

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                     ç¬¬ä¸€é˜¶æ®µï¼šç¼–è¯‘å™¨è¯„ä¼°                          â”‚
   â”‚ evaluate_llm_fixes_merged()                                   â”‚
   â”‚ â€¢ ä½¿ç”¨LLMç”Ÿæˆä¿®å¤ä»£ç                                            â”‚
   â”‚ â€¢ é€šè¿‡GCCç¼–è¯‘å™¨éªŒè¯ä¿®å¤æ•ˆæœ                                      â”‚
   â”‚ â€¢ ç»Ÿè®¡ç¼–è¯‘æˆåŠŸç‡                                               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                    ç¬¬äºŒé˜¶æ®µï¼šLLMè£åˆ¤è¯„ä¼°                         â”‚
   â”‚ Judge_compile()                                               â”‚
   â”‚ â€¢ ä½¿ç”¨LLM-as-Judgeåˆ†æä¿®å¤è´¨é‡                                 â”‚
   â”‚ â€¢ åŒºåˆ†çœŸæ­£ä¿®å¤vsç®€å•åˆ é™¤                                        â”‚
   â”‚ â€¢ æä¾›ç½®ä¿¡åº¦è¯„åˆ†å’Œè¯¦ç»†ç†ç”±                                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                    ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®åˆ†æç»Ÿè®¡                        â”‚
   â”‚ data_analyze()                                                â”‚
   â”‚ â€¢ å…¨é¢ç»Ÿè®¡ç¼–è¯‘æˆåŠŸç‡å’Œä¿®å¤è´¨é‡                                   â”‚
   â”‚ â€¢ æŒ‰æ•°æ®æºã€é”™è¯¯ç±»å‹ã€ç½®ä¿¡åº¦äº¤å‰åˆ†æ                             â”‚
   â”‚ â€¢ ç”Ÿæˆè¯¦ç»†çš„å¯è§†åŒ–æŠ¥å‘Š                                          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸš€ ä¸»è¦å‡½æ•°è¯´æ˜ï¼š

1. evaluate_llm_fixes_merged()
   åŠŸèƒ½ï¼šç¬¬ä¸€é˜¶æ®µç¼–è¯‘å™¨è¯„ä¼°
   è¾“å…¥ï¼šmergedæ ¼å¼çš„é”™è¯¯æ•°æ®é›†
   è¾“å‡ºï¼šç¼–è¯‘è¯„ä¼°ç»“æœï¼ˆæˆåŠŸ/å¤±è´¥ + è¯¦ç»†ä¿¡æ¯ï¼‰
   
   å‚æ•°è¯´æ˜ï¼š
   - input_file: åŒ…å«é”™è¯¯ä»£ç çš„æ•°æ®æ–‡ä»¶
   - output_file: ç¼–è¯‘è¯„ä¼°ç»“æœè¾“å‡ºæ–‡ä»¶
   - base_url: ä¿®å¤æ¨¡å‹APIåœ°å€
   - headers: APIè®¤è¯ä¿¡æ¯
   - max_workers: å¹¶å‘çº¿ç¨‹æ•°

2. Judge_compile()
   åŠŸèƒ½ï¼šç¬¬äºŒé˜¶æ®µLLMè£åˆ¤è¯„ä¼°
   è¾“å…¥ï¼šç¼–è¯‘è¯„ä¼°ç»“æœ + åŸå§‹é”™è¯¯æ•°æ®
   è¾“å‡ºï¼šä¿®å¤è´¨é‡åˆ¤æ–­ç»“æœ
   
   ä¸¤ç§ä½¿ç”¨æ–¹å¼ï¼š
   æ–¹å¼A - ç›´æ¥ä½¿ç”¨å‡½æ•°è¿”å›æ•°æ®ï¼š
   ```python
   evaluation_data = evaluate_llm_fixes_merged(...)
   Judge_compile(
       evaluation_data=evaluation_data,
       original_data_file="åŸå§‹æ•°æ®.json",
       output_file="è£åˆ¤ç»“æœ.json",
       base_url="è£åˆ¤æ¨¡å‹API",
       headers=headers
   )
   ```
   
   æ–¹å¼B - ä»æ–‡ä»¶åŠ è½½æ•°æ®ï¼š
   ```python
   Judge_compile(
       evaluation_file="ç¼–è¯‘è¯„ä¼°ç»“æœ.json",
       original_data_file="åŸå§‹æ•°æ®.json",
       output_file="è£åˆ¤ç»“æœ.json",
       base_url="è£åˆ¤æ¨¡å‹API",
       headers=headers
   )
   ```

3. data_analyze()
   åŠŸèƒ½ï¼šç¬¬ä¸‰é˜¶æ®µæ•°æ®åˆ†æå’Œç»Ÿè®¡
   è¾“å…¥ï¼šLLMè£åˆ¤è¯„ä¼°ç»“æœæ–‡ä»¶
   è¾“å‡ºï¼šè¯¦ç»†çš„ç»Ÿè®¡åˆ†ææŠ¥å‘Š
   
   å‚æ•°è¯´æ˜ï¼š
   - judge_result_file: Judge_compileç”Ÿæˆçš„ç»“æœæ–‡ä»¶è·¯å¾„
   - output_file: ç»Ÿè®¡ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
   - show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
   
   ä½¿ç”¨æ–¹å¼ï¼š
   ```python
   data_analyze(
       judge_result_file="llm_judge_results.json",
       output_file="analysis_results.json",
       show_details=True
   )
   ```
   
   ç»Ÿè®¡å†…å®¹ï¼š
   - ç¼–è¯‘æˆåŠŸç‡åˆ†æ
   - LLMè£åˆ¤ç»“æœåˆ†å¸ƒ
   - æ•°æ®æºå¯¹æ¯”åˆ†æ
   - é”™è¯¯ç±»å‹æˆåŠŸç‡æ’å
   - ç½®ä¿¡åº¦ç»Ÿè®¡åˆ†æ
   - äº¤å‰ç»Ÿè®¡åˆ†æ

ğŸ·ï¸ åˆ¤æ–­ç»“æœåˆ†ç±»ï¼š
   
   âœ… çœŸæ­£ä¿®å¤ï¼šæ­£ç¡®ä¿®å¤ç¼–è¯‘é”™è¯¯ï¼Œä¿æŒåŸæœ‰åŠŸèƒ½é€»è¾‘
      ä¾‹ï¼šæ·»åŠ ç¼ºå¤±å¤´æ–‡ä»¶ã€ä¿®æ­£è¯­æ³•é”™è¯¯ã€ä¿®å¤ç±»å‹åŒ¹é…ç­‰
   
   âŒ ç®€å•åˆ é™¤ï¼šé€šè¿‡åˆ é™¤å‡ºé”™ä»£ç é¿å…ç¼–è¯‘é”™è¯¯ï¼Œä½†ä¸¢å¤±åŸæœ‰åŠŸèƒ½
      ä¾‹ï¼šåˆ é™¤æ•´ä¸ªå‡½æ•°ã€æ³¨é‡Šæ‰å‡ºé”™è¡Œã€ç§»é™¤å˜é‡å£°æ˜ç­‰
   
   âš ï¸ è¿‡åº¦ä¿®æ”¹ï¼šå¤§å¹…æ”¹å˜åŸæœ‰é€»è¾‘æˆ–æ·»åŠ ä¸å¿…è¦çš„ä»£ç 
      ä¾‹ï¼šå®Œå…¨é‡å†™å‡½æ•°é€»è¾‘ã€æ·»åŠ æ— å…³åŠŸèƒ½ç­‰
   
   ğŸš« æ— æ•ˆä¿®å¤ï¼šä¿®å¤ä¸æ­£ç¡®æˆ–å¯èƒ½å¼•å…¥æ–°çš„ç¼–è¯‘é—®é¢˜
      ä¾‹ï¼šè¯­æ³•ä»æœ‰é”™è¯¯ã€é€»è¾‘çŸ›ç›¾ç­‰

ğŸ“Š è¾“å‡ºæ–‡ä»¶æ ¼å¼ï¼š

1. ç¼–è¯‘è¯„ä¼°ç»“æœæ–‡ä»¶ï¼š
   ```json
   {
     "evaluation_results": [
       {
         "uuid": "å”¯ä¸€æ ‡è¯†",
         "error_type": "é”™è¯¯ç±»å‹",
         "success": true/false,
         "original_code": "åŸå§‹é”™è¯¯ä»£ç ",
         "fixed_code": "ä¿®å¤åä»£ç ",
         "compilation_result": "ç¼–è¯‘è¯¦æƒ…"
       }
     ],
     "statistics": {...}
   }
   ```

2. LLMè£åˆ¤è¯„ä¼°ç»“æœæ–‡ä»¶ï¼š
   ```json
   {
     "judge_results": [
       {
         "uuid": "å”¯ä¸€æ ‡è¯†",
         "judge_result": "çœŸæ­£ä¿®å¤/ç®€å•åˆ é™¤/è¿‡åº¦ä¿®æ”¹/æ— æ•ˆä¿®å¤",
         "confidence": 85,
         "reason": "è¯¦ç»†åˆ¤æ–­ç†ç”±",
         "raw_judge_response": "LLMåŸå§‹å“åº”"
       }
     ],
     "judge_statistics": {...}
   }
   ```

3. æ•°æ®åˆ†æç»“æœæ–‡ä»¶ï¼š
   ```json
   {
     "åŸºæœ¬ç»Ÿè®¡": {
       "æ€»æ ·æœ¬æ•°": 1000,
       "ç¼–è¯‘æˆåŠŸç»Ÿè®¡": {"æˆåŠŸ": 800, "å¤±è´¥": 200},
       "è£åˆ¤ç»“æœç»Ÿè®¡": {"çœŸæ­£ä¿®å¤": 600, "ç®€å•åˆ é™¤": 150, ...},
       "æ•°æ®æºç»Ÿè®¡": {"llm_examples": 500, "error_base": 500},
       "é”™è¯¯ç±»å‹ç»Ÿè®¡": {"C2065": 100, "C2009": 80, ...},
       "ç½®ä¿¡åº¦ç»Ÿè®¡": {"å¹³å‡å€¼": 75.5, "ä¸­ä½æ•°": 80, ...}
     },
     "äº¤å‰ç»Ÿè®¡": {
       "æŒ‰æ•°æ®æºçš„ç¼–è¯‘æˆåŠŸæƒ…å†µ": {...},
       "æŒ‰æ•°æ®æºçš„è£åˆ¤ç»“æœ": {...},
       "æŒ‰é”™è¯¯ç±»å‹çš„è£åˆ¤ç»“æœ": {...}
     }
   }
   ```

ğŸ”§ è¿è¡Œç¤ºä¾‹ï¼š

1. å®Œæ•´æµç¨‹ï¼ˆæ¨èï¼‰ï¼š
   ```python
   python evaluate_llm_merged_with_LLM_as_Judge.py
   # æˆ–è€…åœ¨ä»£ç ä¸­è°ƒç”¨ main()
   ```
   
2. ä»…è¿è¡ŒLLMè£åˆ¤è¯„ä¼°ï¼š
   ```python
   # ä¿®æ”¹__main__éƒ¨åˆ†ä¸ºï¼š
   demo_judge_from_file()
   ```

3. ä»…è¿è¡Œæ•°æ®åˆ†æï¼š
   ```python
   # ä¿®æ”¹__main__éƒ¨åˆ†ä¸ºï¼š
   demo_data_analyze()
   ```
   
4. ä¸‰é˜¶æ®µå®Œæ•´æµç¨‹ï¼š
   ```python
   # ç¬¬ä¸€é˜¶æ®µï¼šç¼–è¯‘å™¨è¯„ä¼°
   evaluation_data = evaluate_llm_fixes_merged(...)
   
   # ç¬¬äºŒé˜¶æ®µï¼šLLMè£åˆ¤è¯„ä¼°
   Judge_compile(evaluation_data=evaluation_data, ...)
   
   # ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®åˆ†æ
   data_analyze(judge_result_file="judge_results.json")
   ```ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯ï¼š
   ç³»ç»Ÿä¼šè‡ªåŠ¨ç”Ÿæˆè¯¦ç»†ç»Ÿè®¡ï¼ŒåŒ…æ‹¬ï¼š
   - æ€»ä½“æˆåŠŸç‡å’Œå„é”™è¯¯ç±»å‹æˆåŠŸç‡
   - LLMè£åˆ¤ç»“æœåˆ†å¸ƒ
   - æŒ‰æ•°æ®æºï¼ˆllm_examples vs error_baseï¼‰åˆ†ç»„ç»Ÿè®¡
   - ç½®ä¿¡åº¦åˆ†æï¼ˆå¹³å‡å€¼ã€ä¸­ä½æ•°ï¼‰
   - äº¤å‰ç»Ÿè®¡åˆ†æï¼ˆæ•°æ®æºvsç¼–è¯‘æˆåŠŸç‡ã€é”™è¯¯ç±»å‹vsä¿®å¤è´¨é‡ç­‰ï¼‰
   - Topé”™è¯¯ç±»å‹çš„ä¿®å¤è´¨é‡æ’å
   - å¯è§†åŒ–çš„ç™¾åˆ†æ¯”åˆ†å¸ƒå›¾è¡¨

âš™ï¸ é…ç½®è¯´æ˜ï¼š
   - ä¿®å¤æ¨¡å‹APIï¼šç”¨äºç”Ÿæˆä¿®å¤ä»£ç çš„LLMæœåŠ¡
   - è£åˆ¤æ¨¡å‹APIï¼šç”¨äºè¯„ä¼°ä¿®å¤è´¨é‡çš„LLMæœåŠ¡ï¼ˆå¯ä»¥æ˜¯åŒä¸€ä¸ªï¼‰
   - å¹¶å‘æ•°ï¼šæ ¹æ®æœåŠ¡å™¨æ€§èƒ½è°ƒæ•´ï¼Œå»ºè®®8-24
   - è¾“å…¥æ•°æ®ï¼šéœ€è¦æ˜¯mergedæ ¼å¼ï¼ŒåŒ…å«llm_examples_dataå’Œerror_base_data

ğŸ’¡ ä½¿ç”¨å»ºè®®ï¼š
   1. ç¡®ä¿è¾“å…¥æ•°æ®æ ¼å¼æ­£ç¡®
   2. æ ¹æ®APIæœåŠ¡å™¨æ€§èƒ½è°ƒæ•´å¹¶å‘æ•°
   3. å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ¨¡å‹è¿›è¡Œä¿®å¤å’Œè£åˆ¤
   4. æ³¨æ„APIè°ƒç”¨é™åˆ¶å’Œå»¶è¿Ÿè®¾ç½®
   5. å®šæœŸæ£€æŸ¥è¾“å‡ºæ–‡ä»¶ç¡®ä¿è¯„ä¼°æ­£å¸¸è¿›è¡Œ

===============================================================================
"""

