#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¼–è¯‘é”™è¯¯ç›¸ä¼¼æ€§åˆ†æè„šæœ¬
ä¸“é—¨ç”¨äºæ‰¾å‡ºä»£ç å’Œç¼–è¯‘é”™è¯¯åŸå› ç›¸ä¼¼åº¦è¶…è¿‡90%çš„é‡å¤è¯­æ–™ï¼Œå¹¶å°†ç›¸ä¼¼çš„é”™è¯¯ä¿å­˜åˆ°æ–°æ–‡ä»¶ä¸­
"""

import json
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any, Set, Tuple
from collections import defaultdict, Counter
import difflib
import os
import sys
import re
import hashlib
from math import sqrt

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def create_tfidf_vectors(texts: List[str]) -> Tuple[List[Dict[str, float]], Dict[str, float]]:
    """åˆ›å»ºTF-IDFå‘é‡ï¼ˆç¦»çº¿å®ç°ï¼‰"""
    # é¢„å¤„ç†æ–‡æœ¬
    processed_texts = []
    for text in texts:
        # æå–ä»£ç æ ‡è¯†ç¬¦å’Œå…³é”®è¯
        words = extract_code_features(text)
        processed_texts.append(words)
    
    # æ„å»ºè¯æ±‡è¡¨
    vocab = set()
    for words in processed_texts:
        vocab.update(words)
    vocab = list(vocab)
    
    # è®¡ç®—æ–‡æ¡£é¢‘ç‡
    doc_freq = {}
    total_docs = len(processed_texts)
    for word in vocab:
        doc_freq[word] = sum(1 for words in processed_texts if word in words)
    
    # è®¡ç®—IDF
    idf = {}
    for word in vocab:
        idf[word] = max(0.1, log_safe(total_docs / doc_freq[word]))
    
    # è®¡ç®—TF-IDFå‘é‡
    tfidf_vectors = []
    for words in processed_texts:
        word_count = Counter(words)
        total_words = len(words)
        
        vector = {}
        for word in word_count:
            tf = word_count[word] / max(1, total_words)
            vector[word] = tf * idf[word]
        
        tfidf_vectors.append(vector)
    
    return tfidf_vectors, idf


def extract_code_features(text: str) -> List[str]:
    """ä»ä»£ç æ–‡æœ¬ä¸­æå–ç‰¹å¾è¯"""
    if not text:
        return []
    
    # æå–C++å…³é”®è¯ã€æ ‡è¯†ç¬¦ã€æ“ä½œç¬¦ç­‰
    features = []
    
    # C++å…³é”®è¯
    cpp_keywords = re.findall(r'\b(?:int|char|float|double|void|if|else|for|while|return|include|define|const|static|class|struct|namespace|using|std|cout|cin|endl)\b', text)
    features.extend(cpp_keywords)
    
    # æ ‡è¯†ç¬¦ï¼ˆå˜é‡åã€å‡½æ•°åï¼‰
    identifiers = re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', text)
    features.extend(identifiers)
    
    # æ“ä½œç¬¦å’Œç¬¦å·
    operators = re.findall(r'[+\-*/=<>!&|^%]+|[{}()\[\];,.]', text)
    features.extend(operators)
    
    # å­—ç¬¦ä¸²å­—é¢é‡çš„ç‰¹å¾
    strings = re.findall(r'"[^"]*"', text)
    for s in strings:
        if len(s) > 2:  # ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
            features.append('STRING_LITERAL')
    
    # æ•°å­—å­—é¢é‡
    numbers = re.findall(r'\b\d+\b', text)
    if numbers:
        features.append('NUMBER_LITERAL')
    
    return features


def log_safe(x):
    """å®‰å…¨çš„å¯¹æ•°è®¡ç®—"""
    import math
    return math.log(max(1e-10, x))


def cosine_similarity_sparse(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
    """è®¡ç®—ç¨€ç–å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    if not vec1 or not vec2:
        return 0.0
    
    # è®¡ç®—ç‚¹ç§¯
    dot_product = 0.0
    for word in vec1:
        if word in vec2:
            dot_product += vec1[word] * vec2[word]
    
    # è®¡ç®—å‘é‡æ¨¡é•¿
    norm1 = sqrt(sum(v * v for v in vec1.values()))
    norm2 = sqrt(sum(v * v for v in vec2.values()))
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)


def calculate_offline_similarity_batch(items: List[Dict]) -> List[List[float]]:
    """ä½¿ç”¨ç¦»çº¿ç®—æ³•æ‰¹é‡è®¡ç®—æ‰€æœ‰itemä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ"""
    if not items:
        return []
    
    # å‡†å¤‡æ–‡æœ¬æ•°æ®
    texts = []
    for item in items:
        # ç»„åˆä»£ç å’Œé”™è¯¯ä¿¡æ¯
        code = item.get('error_example_llm_code', '')
        detail = item.get('error_example_llm_detail', '')
        # ç®€åŒ–é”™è¯¯è¯¦æƒ…ï¼ˆå»é™¤æ–‡ä»¶è·¯å¾„ï¼‰
        clean_detail = '\n'.join([line.split(':', 2)[-1] if ':' in line else line 
                                 for line in detail.split('\n')])
        combined_text = f"CODE: {normalize_code(code)} ERROR: {clean_detail}"
        texts.append(combined_text)
    
    # è®¡ç®—TF-IDFå‘é‡
    tfidf_vectors, _ = create_tfidf_vectors(texts)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    n = len(tfidf_vectors)
    similarity_matrix = [[0.0 for _ in range(n)] for _ in range(n)]
    
    for i in range(n):
        for j in range(i, n):
            if i == j:
                similarity_matrix[i][j] = 1.0
            else:
                # ç»„åˆå¤šç§ç›¸ä¼¼åº¦ç®—æ³•
                tfidf_sim = cosine_similarity_sparse(tfidf_vectors[i], tfidf_vectors[j])
                difflib_sim = difflib.SequenceMatcher(None, texts[i], texts[j]).ratio()
                
                # åŠ æƒç»„åˆï¼šTF-IDFæƒé‡æ›´é«˜
                combined_sim = 0.7 * tfidf_sim + 0.3 * difflib_sim
                
                similarity_matrix[i][j] = combined_sim
                similarity_matrix[j][i] = combined_sim
    
    return similarity_matrix


def process_error_group_offline(args):
    """ä½¿ç”¨ç¦»çº¿ç®—æ³•å¤„ç†å•ä¸ªé”™è¯¯ç±»å‹ç»„"""
    error_key, group_items, similarity_threshold, group_idx = args
    
    try:
        processed = set()
        similar_groups = []
        comparisons_made = 0
        
        if len(group_items) < 2:
            return similar_groups, comparisons_made, group_idx
        
        # ç¦»çº¿æ‰¹é‡è®¡ç®—æ¨¡å¼
        items_only = [item for _, item in group_items]
        similarity_matrix = calculate_offline_similarity_batch(items_only)
        
        if similarity_matrix:
            # æ ¹æ®ç›¸ä¼¼åº¦çŸ©é˜µæ‰¾å‡ºç›¸ä¼¼ç»„
            n = len(group_items)
            comparisons_made += n * (n - 1) // 2
            
            for i in range(n):
                orig_idx1, item1 = group_items[i]
                if orig_idx1 in processed:
                    continue
                    
                current_group = [item1]
                processed.add(orig_idx1)
                
                for j in range(i + 1, n):
                    orig_idx2, item2 = group_items[j]
                    if orig_idx2 in processed:
                        continue
                        
                    similarity = similarity_matrix[i][j]
                    
                    if similarity >= similarity_threshold:
                        current_group.append(item2)
                        processed.add(orig_idx2)
                
                # åªä¿ç•™åŒ…å«å¤šä¸ªé¡¹ç›®çš„ç»„
                if len(current_group) > 1:
                    similar_groups.append(current_group)
        
        return similar_groups, comparisons_made, group_idx
        
    except Exception as e:
        print(f"âŒ ç¦»çº¿ç®—æ³•å¤„ç†é”™è¯¯ç»„ {error_key} æ—¶å‡ºé”™: {e}")
        return [], 0, group_idx


def print_progress_bar(iteration, total, prefix='', suffix='', decimals=1, length=50, fill='â–ˆ', print_end="\r"):
    """
    åœ¨æ§åˆ¶å°æ‰“å°è¿›åº¦æ¡
    @params:
        iteration   - Required  : å½“å‰è¿­ä»£ (Int)
        total       - Required  : æ€»è¿­ä»£æ•° (Int)
        prefix      - Optional  : å‰ç¼€å­—ç¬¦ä¸² (Str)
        suffix      - Optional  : åç¼€å­—ç¬¦ä¸² (Str)
        decimals    - Optional  : å°æ•°ç‚¹åä½æ•° (Int)
        length      - Optional  : è¿›åº¦æ¡é•¿åº¦ (Int)
        fill        - Optional  : å¡«å……å­—ç¬¦ (Str)
        print_end   - Optional  : è¡Œå°¾å­—ç¬¦ (Str)
    """
    percent = ("{0:." + str(decimals) + "f}").format(100 * (iteration / float(total)))
    filled_length = int(length * iteration // total)
    bar = fill * filled_length + '-' * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent}% {suffix}', end=print_end)
    # å¦‚æœå®Œæˆäº†ï¼Œæ‰“å°æ–°è¡Œ
    if iteration == total: 
        print()


def load_json_file(file_path: str) -> List[Dict]:
    """åŠ è½½JSONæ–‡ä»¶"""
    try:
        logger.info(f"æ­£åœ¨åŠ è½½æ–‡ä»¶: {file_path}")
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            raise ValueError("JSONæ–‡ä»¶åº”è¯¥åŒ…å«ä¸€ä¸ªæ•°ç»„")
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•")
        return data
    except Exception as e:
        logger.error(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {e}")
        raise


def save_json_file(data: List[Dict], file_path: str) -> None:
    """ä¿å­˜JSONæ–‡ä»¶"""
    try:
        logger.info(f"æ­£åœ¨ä¿å­˜æ–‡ä»¶: {file_path}")
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
    except Exception as e:
        logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
        raise


def normalize_code(code: str) -> str:
    """æ ‡å‡†åŒ–ä»£ç ï¼Œå»é™¤ç©ºç™½å­—ç¬¦å’Œæ³¨é‡Š"""
    if not code:
        return ""
    
    lines = code.split('\n')
    normalized_lines = []
    
    for line in lines:
        # å»é™¤è¡Œæ³¨é‡Š
        if '//' in line:
            line = line.split('//')[0]
        
        # å»é™¤å—æ³¨é‡Šçš„å¼€å§‹éƒ¨åˆ†
        if '/*' in line:
            line = line.split('/*')[0]
        
        # å»é™¤å‰åç©ºç™½
        line = line.strip()
        
        # å»é™¤å¤šä½™çš„ç©ºæ ¼
        line = ' '.join(line.split())
        
        if line:
            normalized_lines.append(line)
    
    return '\n'.join(normalized_lines)


def calculate_code_similarity(code1: str, code2: str) -> float:
    """è®¡ç®—ä¸¤æ®µä»£ç çš„ç›¸ä¼¼åº¦"""
    if not code1 or not code2:
        return 0.0
    
    # æ ‡å‡†åŒ–ä»£ç ï¼šå»é™¤ç©ºç™½å­—ç¬¦å’Œæ³¨é‡Š
    normalized_code1 = normalize_code(code1)
    normalized_code2 = normalize_code(code2)
    
    if not normalized_code1 or not normalized_code2:
        return 0.0
    
    # ä½¿ç”¨difflibè®¡ç®—ç›¸ä¼¼åº¦
    similarity = difflib.SequenceMatcher(None, normalized_code1, normalized_code2).ratio()
    return similarity


def calculate_error_detail_similarity(detail1: str, detail2: str) -> float:
    """è®¡ç®—é”™è¯¯è¯¦ç»†ä¿¡æ¯çš„ç›¸ä¼¼åº¦"""
    if not detail1 or not detail2:
        return 0.0
    
    # å»é™¤ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼Œåªæ¯”è¾ƒé”™è¯¯ä¿¡æ¯æœ¬èº«
    lines1 = [line.split(':', 3)[-1] if ':' in line else line for line in detail1.split('\n')]
    lines2 = [line.split(':', 3)[-1] if ':' in line else line for line in detail2.split('\n')]
    
    detail1_clean = '\n'.join(lines1).strip()
    detail2_clean = '\n'.join(lines2).strip()
    
    if not detail1_clean or not detail2_clean:
        return 0.0
    
    similarity = difflib.SequenceMatcher(None, detail1_clean, detail2_clean).ratio()
    return similarity


def calculate_overall_similarity(item1: Dict, item2: Dict) -> float:
    """è®¡ç®—ä¸¤ä¸ªé”™è¯¯æ¡ç›®çš„æ•´ä½“ç›¸ä¼¼åº¦ï¼ˆä¸“æ³¨äºä»£ç å’Œç¼–è¯‘é”™è¯¯åŸå› ï¼‰- ç¦»çº¿ç‰ˆæœ¬"""
    # æƒé‡è®¾ç½® - æ›´ä¸“æ³¨äºä»£ç å’Œé”™è¯¯åŸå› 
    code_weight = 0.6        # ä»£ç ç›¸ä¼¼åº¦æƒé‡
    detail_weight = 0.3      # ç¼–è¯‘é”™è¯¯è¯¦æƒ…æƒé‡
    error_type_weight = 0.1  # é”™è¯¯ç±»å‹æƒé‡
    
    # è®¡ç®—ä»£ç ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨å¢å¼ºç‰ˆæœ¬ï¼‰
    code1 = item1.get('error_example_llm_code', '')
    code2 = item2.get('error_example_llm_code', '')
    code_sim = calculate_enhanced_code_similarity(code1, code2)
    
    # è®¡ç®—é”™è¯¯è¯¦æƒ…ç›¸ä¼¼åº¦
    detail_sim = calculate_error_detail_similarity(
        item1.get('error_example_llm_detail', ''),
        item2.get('error_example_llm_detail', '')
    )
    
    # é”™è¯¯ç±»å‹ç›¸ä¼¼åº¦ï¼ˆç›¸åŒä¸º1ï¼Œä¸åŒä¸º0ï¼‰
    error_type_sim = 1.0 if (item1.get('error_type') == item2.get('error_type') and 
                             item1.get('error_type_detail') == item2.get('error_type_detail')) else 0.0
    
    # åŠ æƒè®¡ç®—æ€»ç›¸ä¼¼åº¦
    overall_similarity = (code_sim * code_weight + 
                         detail_sim * detail_weight + 
                         error_type_sim * error_type_weight)
    
    return overall_similarity


def calculate_enhanced_code_similarity(code1: str, code2: str) -> float:
    """å¢å¼ºç‰ˆä»£ç ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆä½¿ç”¨TF-IDF + difflibï¼‰"""
    if not code1 or not code2:
        return 0.0
    
    # æ ‡å‡†åŒ–ä»£ç 
    norm_code1 = normalize_code(code1)
    norm_code2 = normalize_code(code2)
    
    if not norm_code1 or not norm_code2:
        return 0.0
    
    # ä½¿ç”¨TF-IDFè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
    texts = [norm_code1, norm_code2]
    tfidf_vectors, _ = create_tfidf_vectors(texts)
    
    if len(tfidf_vectors) >= 2:
        tfidf_sim = cosine_similarity_sparse(tfidf_vectors[0], tfidf_vectors[1])
    else:
        tfidf_sim = 0.0
    
    # ä½¿ç”¨difflibè®¡ç®—åºåˆ—ç›¸ä¼¼åº¦
    difflib_sim = difflib.SequenceMatcher(None, norm_code1, norm_code2).ratio()
    
    # è®¡ç®—ä»£ç ç»“æ„ç›¸ä¼¼åº¦ï¼ˆåŸºäºè¡Œæ•°ã€æ‹¬å·ç­‰ï¼‰
    structure_sim = calculate_code_structure_similarity(norm_code1, norm_code2)
    
    # åŠ æƒç»„åˆå¤šç§ç›¸ä¼¼åº¦
    combined_sim = 0.5 * tfidf_sim + 0.3 * difflib_sim + 0.2 * structure_sim
    
    return combined_sim


def calculate_code_structure_similarity(code1: str, code2: str) -> float:
    """è®¡ç®—ä»£ç ç»“æ„ç›¸ä¼¼åº¦"""
    if not code1 or not code2:
        return 0.0
    
    # æå–ç»“æ„ç‰¹å¾
    features1 = extract_structure_features(code1)
    features2 = extract_structure_features(code2)
    
    # è®¡ç®—ç‰¹å¾å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
    all_features = set(features1.keys()) | set(features2.keys())
    if not all_features:
        return 0.0
    
    vec1 = [features1.get(f, 0) for f in all_features]
    vec2 = [features2.get(f, 0) for f in all_features]
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    norm1 = sqrt(sum(a * a for a in vec1))
    norm2 = sqrt(sum(b * b for b in vec2))
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)


def extract_structure_features(code: str) -> Dict[str, int]:
    """æå–ä»£ç ç»“æ„ç‰¹å¾"""
    features = {}
    
    # è¡Œæ•°
    features['line_count'] = len(code.split('\n'))
    
    # å„ç§æ‹¬å·æ•°é‡
    features['curly_braces'] = code.count('{') + code.count('}')
    features['parentheses'] = code.count('(') + code.count(')')
    features['square_brackets'] = code.count('[') + code.count(']')
    
    # åˆ†å·æ•°é‡ï¼ˆè¯­å¥æ•°ï¼‰
    features['semicolons'] = code.count(';')
    
    # å…³é”®è¯æ•°é‡
    keywords = ['if', 'else', 'for', 'while', 'return', 'int', 'char', 'float', 'double']
    for keyword in keywords:
        features[f'keyword_{keyword}'] = len(re.findall(r'\b' + keyword + r'\b', code))
    
    # æ“ä½œç¬¦æ•°é‡
    features['assignments'] = code.count('=')
    features['comparisons'] = code.count('==') + code.count('!=') + code.count('<=') + code.count('>=')
    
    return features


def find_similar_groups(data: List[Dict], similarity_threshold: float = 0.90, use_gpu: bool = False, gpu_ids: List[int] = None) -> List[List[Dict]]:
    """æ‰¾å‡ºä»£ç å’Œç¼–è¯‘é”™è¯¯åŸå› ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼çš„é”™è¯¯ç»„ï¼ˆæŒ‰error_typeåˆ†ç»„ä¼˜åŒ–ï¼‰"""
    logger.info(f"å¼€å§‹æŸ¥æ‰¾ä»£ç å’Œç¼–è¯‘é”™è¯¯åŸå› ç›¸ä¼¼åº¦è¶…è¿‡ {similarity_threshold*100:.0f}% çš„è¯­æ–™...")
    
    # æŒ‰error_typeåˆ†ç»„ä»¥ä¼˜åŒ–æ¯”è¾ƒæ•ˆç‡
    print(f"\nğŸ”§ ä¼˜åŒ–ç­–ç•¥: æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„æ¯”è¾ƒï¼Œå¤§å¹…æå‡æ•ˆç‡...")
    error_type_groups = defaultdict(list)
    
    for i, item in enumerate(data):
        error_key = f"{item.get('error_type', '')}_{item.get('error_type_detail', '')}"
        error_type_groups[error_key].append((i, item))
    
    print(f"ğŸ“Š åˆ†ç»„ç»“æœ: {len(data):,} æ¡è¯­æ–™åˆ†ä¸º {len(error_type_groups)} ä¸ªé”™è¯¯ç±»å‹ç»„")
    
    # è®¡ç®—ä¼˜åŒ–åçš„æ¯”è¾ƒæ¬¡æ•°
    estimated_comparisons = 0
    for group_items in error_type_groups.values():
        n = len(group_items)
        if n > 1:
            estimated_comparisons += n * (n - 1) // 2
    
    print(f"âš¡ ä¼˜åŒ–æ•ˆæœ: æ¯”è¾ƒæ¬¡æ•°ä» {len(data)*(len(data)-1)//2:,} å‡å°‘åˆ° {estimated_comparisons:,}")
    print(f"ğŸš€ æ•ˆç‡æå‡: {(len(data)*(len(data)-1)//2) / max(1, estimated_comparisons):.1f}x å€")
    
    processed = set()
    similar_groups = []
    comparisons_made = 0
    
    import time
    start_time = time.time()
    last_update_time = start_time
    processed_groups = 0
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if use_gpu:
        if gpu_ids is None:
            gpu_ids = list(range(8))  # é»˜è®¤ä½¿ç”¨0-7å¡
        
        # éªŒè¯GPUå¯ç”¨æ€§
        import torch
        if not torch.cuda.is_available():
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
            use_gpu = False
        else:
            available_gpus = list(range(torch.cuda.device_count()))
            valid_gpu_ids = [gpu_id for gpu_id in gpu_ids if gpu_id in available_gpus]
            if not valid_gpu_ids:
                print(f"âš ï¸  æŒ‡å®šçš„GPU {gpu_ids} éƒ½ä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
                use_gpu = False
            else:
                gpu_ids = valid_gpu_ids
                print(f"ğŸ” éªŒè¯GPUå¯ç”¨æ€§: {gpu_ids}")
    
    print(f"\nğŸ” å¼€å§‹æŒ‰é”™è¯¯ç±»å‹ç»„è¿›è¡Œç›¸ä¼¼åº¦åˆ†æ...")
    if use_gpu:
        print(f"ğŸš€ å¤šGPUå¹¶è¡Œæ¨¡å¼: ä½¿ç”¨ {len(gpu_ids)} å¼ GPUå¡ {gpu_ids}")
    else:
        print(f"ğŸ’» CPUä¼ ç»Ÿæ¨¡å¼")
    
    # æŒ‰é”™è¯¯ç±»å‹ç»„è¿›è¡Œæ¯”è¾ƒ
    if use_gpu and gpu_ids:
        # å¤šGPUå¹¶è¡Œå¤„ç†æ¨¡å¼
        import threading
        from queue import Queue
        
        # å‡†å¤‡ä»»åŠ¡é˜Ÿåˆ—
        task_queue = Queue()
        result_queue = Queue()
        
        # åªå¤„ç†æœ‰å¤šä¸ªå…ƒç´ çš„ç»„
        valid_groups = [(k, v) for k, v in error_type_groups.items() if len(v) >= 2]
        print(f"ğŸ“‹ å‡†å¤‡å¤„ç† {len(valid_groups)} ä¸ªæœ‰æ•ˆé”™è¯¯ç±»å‹ç»„")
        
        # ä¸ºæ¯ä¸ªé”™è¯¯ç»„åˆ›å»ºä»»åŠ¡
        for group_idx, (error_key, group_items) in enumerate(valid_groups):
            task_queue.put((error_key, group_items, similarity_threshold, group_idx))
        
        def gpu_worker(device_id, worker_id):
            """GPUå·¥ä½œçº¿ç¨‹"""
            while True:
                try:
                    task = task_queue.get(timeout=1)
                    if task is None:
                        break
                    
                    error_key, group_items, threshold, group_idx = task
                    
                    # å¤„ç†è¿™ä¸ªé”™è¯¯ç»„
                    group_similar, group_comparisons, _ = process_error_group_on_gpu(
                        (error_key, group_items, threshold, device_id, group_idx)
                    )
                    
                    result_queue.put((group_similar, group_comparisons, error_key, group_idx))
                    task_queue.task_done()
                    
                except:
                    break
        
        # å¯åŠ¨å¤šä¸ªGPUå·¥ä½œçº¿ç¨‹
        threads = []
        for i, device_id in enumerate(gpu_ids):
            thread = threading.Thread(target=gpu_worker, args=(device_id, i))
            thread.daemon = True
            thread.start()
            threads.append(thread)
            print(f"ğŸ”¥ å¯åŠ¨GPUå·¥ä½œçº¿ç¨‹ {i} (è®¾å¤‡: cuda:{device_id})")
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
        processed_groups = 0
        total_comparisons = 0
        
        while processed_groups < len(valid_groups):
            try:
                group_similar, group_comparisons, error_key, group_idx = result_queue.get(timeout=5)
                
                # åˆå¹¶ç»“æœ
                similar_groups.extend(group_similar)
                total_comparisons += group_comparisons
                processed_groups += 1
                
                if group_similar:
                    print(f"âœ… GPUå¤„ç†ç»„ {error_key}: å‘ç° {len(group_similar)} ä¸ªç›¸ä¼¼ç»„")
                
                # æ›´æ–°è¿›åº¦æ¡
                current_time = time.time()
                elapsed_time = current_time - start_time
                
                if processed_groups > 0:
                    avg_time_per_group = elapsed_time / processed_groups
                    remaining_groups = len(valid_groups) - processed_groups
                    estimated_remaining_time = remaining_groups * avg_time_per_group
                    
                    if estimated_remaining_time > 60:
                        time_str = f"{estimated_remaining_time/60:.1f}åˆ†é’Ÿ"
                    else:
                        time_str = f"{estimated_remaining_time:.0f}ç§’"
                else:
                    time_str = "è®¡ç®—ä¸­..."
                
                progress_info = f"å·²å®Œæˆ {processed_groups}/{len(valid_groups)} | ç›¸ä¼¼ç»„ {len(similar_groups)} | å‰©ä½™ {time_str}"
                print_progress_bar(processed_groups, len(valid_groups), 
                                 prefix='ğŸš€ å¤šGPUå¹¶è¡Œè¿›åº¦:', suffix=progress_info, length=60)
                
            except:
                break
        
        # åœæ­¢æ‰€æœ‰çº¿ç¨‹
        for _ in threads:
            task_queue.put(None)
        
        for thread in threads:
            thread.join()
        
        comparisons_made = total_comparisons
        processed_groups = len(valid_groups)
        
        print(f"\nğŸ¯ å¤šGPUå¹¶è¡Œå¤„ç†å®Œæˆ!")
        print(f"   ä½¿ç”¨GPU: {gpu_ids}")
        print(f"   å¤„ç†ç»„æ•°: {len(valid_groups)}")
        print(f"   å‘ç°ç›¸ä¼¼ç»„: {len(similar_groups)}")
        
    else:
        # CPUä¼ ç»Ÿè®¡ç®—æ¨¡å¼
        for error_key, group_items in error_type_groups.items():
            if len(group_items) < 2:
                continue  # è·³è¿‡åªæœ‰ä¸€ä¸ªå…ƒç´ çš„ç»„
                
            print(f"\nğŸ“ å¤„ç†é”™è¯¯ç±»å‹: {error_key} ({len(group_items)} æ¡è¯­æ–™)")
            processed_groups += 1
            
            # CPUä¼ ç»Ÿè®¡ç®—æ¨¡å¼
            for i in range(len(group_items)):
                orig_idx1, item1 = group_items[i]
                if orig_idx1 in processed:
                    continue
                    
                current_group = [item1]
                processed.add(orig_idx1)
                
                # ä¸åŒç»„å†…åç»­é¡¹ç›®æ¯”è¾ƒ
                for j in range(i + 1, len(group_items)):
                    orig_idx2, item2 = group_items[j]
                    if orig_idx2 in processed:
                        continue
                        
                    similarity = calculate_overall_similarity(item1, item2)
                    comparisons_made += 1
                    
                    if similarity >= similarity_threshold:
                        current_group.append(item2)
                        processed.add(orig_idx2)
                        logger.debug(f"å‘ç°ç›¸ä¼¼è¯­æ–™: UUID {item1.get('uuid')} ä¸ {item2.get('uuid')}, ç›¸ä¼¼åº¦: {similarity:.3f}")
                
                # åªä¿ç•™åŒ…å«å¤šä¸ªé¡¹ç›®çš„ç»„ï¼ˆå³çœŸæ­£çš„é‡å¤ç»„ï¼‰
                if len(current_group) > 1:
                    similar_groups.append(current_group)
                    logger.info(f"å‘ç°ç›¸ä¼¼ç»„ {len(similar_groups)}: åŒ…å« {len(current_group)} ä¸ªç›¸ä¼¼è¯­æ–™ (é”™è¯¯ç±»å‹: {error_key})")
            
            # æ›´æ–°è¿›åº¦æ¡
            current_time = time.time()
            if (current_time - last_update_time) >= 1.0:
                # è®¡ç®—é¢„è®¡å‰©ä½™æ—¶é—´
                elapsed_time = current_time - start_time
                if processed_groups > 0:
                    avg_time_per_group = elapsed_time / processed_groups
                    remaining_groups = len(error_type_groups) - processed_groups
                    estimated_remaining_time = remaining_groups * avg_time_per_group
                    
                    # æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º
                    if estimated_remaining_time > 3600:
                        time_str = f"{estimated_remaining_time/3600:.1f}å°æ—¶"
                    elif estimated_remaining_time > 60:
                        time_str = f"{estimated_remaining_time/60:.1f}åˆ†é’Ÿ"
                    else:
                        time_str = f"{estimated_remaining_time:.0f}ç§’"
                else:
                    time_str = "è®¡ç®—ä¸­..."
                
                # æ›´æ–°è¿›åº¦æ¡
                progress_info = f"ç±»å‹ç»„ {processed_groups}/{len([k for k, v in error_type_groups.items() if len(v) >= 2])} | ç›¸ä¼¼ç»„ {len(similar_groups)} | å‰©ä½™ {time_str}"
                print_progress_bar(processed_groups, len([k for k, v in error_type_groups.items() if len(v) >= 2]), 
                                 prefix='ğŸ’» CPUä¼˜åŒ–åˆ†æè¿›åº¦:', suffix=progress_info, length=60)
                
                last_update_time = current_time
    
    # æœ€ç»ˆè¿›åº¦æ¡æ›´æ–°
    elapsed_time = time.time() - start_time
    print_progress_bar(len(error_type_groups), len(error_type_groups), 
                     prefix=f'{"å¤šGPUå¹¶è¡Œ" if use_gpu else "CPUä¼˜åŒ–"}åˆ†æè¿›åº¦:', suffix=f"å®Œæˆï¼è€—æ—¶ {elapsed_time:.1f}ç§’", length=60)
    
    print(f"\nâœ… {'å¤šGPUå¹¶è¡Œ' if use_gpu else 'CPUä¼˜åŒ–'}åˆ†æå®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ€»è¯­æ–™æ•°: {len(data):,}")
    print(f"   - é”™è¯¯ç±»å‹ç»„æ•°: {len(error_type_groups)}")
    if use_gpu:
        print(f"   - è®¡ç®—æ¨¡å¼: å¤šGPUå¹¶è¡Œå¤„ç†")
        print(f"   - ä½¿ç”¨GPU: {gpu_ids}")
    else:
        print(f"   - è®¡ç®—æ¨¡å¼: CPUé€å¯¹è®¡ç®—")
        print(f"   - å®é™…æ¯”è¾ƒæ¬¡æ•°: {comparisons_made:,}")
        print(f"   - å¹³å‡æ¯”è¾ƒé€Ÿåº¦: {comparisons_made/elapsed_time:.0f} æ¬¡/ç§’")
        print(f"   - æ•ˆç‡æå‡: {(len(data)*(len(data)-1)//2) / max(1, comparisons_made):.1f}x å€")
    print(f"   - å‘ç°ç›¸ä¼¼ç»„: {len(similar_groups)}")
    print(f"   - æ€»è€—æ—¶: {elapsed_time:.1f}ç§’")
    
    return similar_groups


def analyze_similarity_distribution(data: List[Dict], sample_size: int = 1000) -> Dict:
    """åˆ†æä»£ç å’Œç¼–è¯‘é”™è¯¯åŸå› ç›¸ä¼¼åº¦åˆ†å¸ƒæƒ…å†µï¼ˆé‡‡æ ·åˆ†æä»¥æé«˜æ•ˆç‡ï¼‰"""
    logger.info(f"æ­£åœ¨åˆ†æä»£ç å’Œç¼–è¯‘é”™è¯¯åŸå› ç›¸ä¼¼åº¦åˆ†å¸ƒï¼ˆé‡‡æ · {sample_size} å¯¹ï¼‰...")
    
    import random
    
    # å¦‚æœæ•°æ®é‡å¤§ï¼Œè¿›è¡Œé‡‡æ ·
    if len(data) > sample_size:
        sample_indices = random.sample(range(len(data)), min(sample_size, len(data)))
        sample_data = [data[i] for i in sample_indices]
    else:
        sample_data = data
    
    similarities = []
    high_similarity_pairs = []
    total_comparisons = 0
    
    print(f"\næ­£åœ¨é‡‡æ ·åˆ†æ {len(sample_data)} æ¡è¯­æ–™çš„ç›¸ä¼¼åº¦åˆ†å¸ƒ...")
    
    import time
    start_time = time.time()
    
    for i in range(len(sample_data)):
        comparison_count = min(10, len(sample_data) - i - 1)  # é™åˆ¶æ¯”è¾ƒèŒƒå›´ä»¥æé«˜æ•ˆç‡
        for j in range(i + 1, i + 1 + comparison_count):
            if j >= len(sample_data):
                break
            sim = calculate_overall_similarity(sample_data[i], sample_data[j])
            similarities.append(sim)
            total_comparisons += 1
            
            if sim > 0.8:  # è®°å½•é«˜ç›¸ä¼¼åº¦å¯¹
                high_similarity_pairs.append({
                    'uuid1': sample_data[i].get('uuid'),
                    'uuid2': sample_data[j].get('uuid'),
                    'similarity': sim
                })
        
        # æ›´æ–°è¿›åº¦æ¡
        high_sim_count = len([s for s in similarities if s > 0.9])
        progress_info = f"å·²æ¯”è¾ƒ {total_comparisons} å¯¹ | é«˜ç›¸ä¼¼åº¦: {high_sim_count}"
        print_progress_bar(i + 1, len(sample_data), prefix='é‡‡æ ·åˆ†æè¿›åº¦:', suffix=progress_info, length=60)
    
    # æœ€ç»ˆç»Ÿè®¡
    elapsed_time = time.time() - start_time
    print(f"\nâœ… é‡‡æ ·åˆ†æå®Œæˆï¼è€—æ—¶ {elapsed_time:.1f}ç§’")
    
    if not similarities:
        return {'error': 'No similarities calculated'}
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    analysis = {
        'total_comparisons': len(similarities),
        'avg_similarity': sum(similarities) / len(similarities),
        'max_similarity': max(similarities),
        'min_similarity': min(similarities),
        'high_similarity_count': len([s for s in similarities if s > 0.9]),
        'very_high_similarity_count': len([s for s in similarities if s > 0.95]),
        'exact_duplicates': len([s for s in similarities if s > 0.99]),
        'high_similarity_examples': high_similarity_pairs[:10]  # å‰10ä¸ªé«˜ç›¸ä¼¼åº¦ç¤ºä¾‹
    }
    
    return analysis


def save_similar_groups(similar_groups: List[List[Dict]], output_dir: str, base_name: str) -> None:
    """å°†ç›¸ä¼¼çš„è¯­æ–™ç»„ä¿å­˜åˆ°ä¸åŒçš„æ–‡ä»¶ä¸­"""
    logger.info(f"æ­£åœ¨ä¿å­˜ {len(similar_groups)} ä¸ªç›¸ä¼¼è¯­æ–™ç»„åˆ° {output_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºæ±‡æ€»ä¿¡æ¯
    summary = {
        'total_groups': len(similar_groups),
        'total_duplicates': sum(len(group) for group in similar_groups),
        'groups_info': []
    }
    
    for i, group in enumerate(similar_groups):
        group_file = f"{base_name}_similar_group_{i+1}.json"
        group_path = os.path.join(output_dir, group_file)
        
        # ä¿å­˜ç›¸ä¼¼ç»„
        save_json_file(group, group_path)
        
        # æ·»åŠ åˆ°æ±‡æ€»ä¿¡æ¯
        group_info = {
            'group_id': i + 1,
            'file': group_file,
            'count': len(group),
            'error_type': group[0].get('error_type', 'unknown'),
            'error_type_detail': group[0].get('error_type_detail', 'unknown'),
            'uuids': [item.get('uuid') for item in group]
        }
        summary['groups_info'].append(group_info)
        
        logger.info(f"ä¿å­˜ç›¸ä¼¼ç»„ {i+1}: {len(group)} ä¸ªç›¸ä¼¼è¯­æ–™ -> {group_file}")
    
    # ä¿å­˜æ±‡æ€»æ–‡ä»¶
    summary_path = os.path.join(output_dir, f"{base_name}_similarity_summary.json")
    save_json_file(summary, summary_path)
    
    logger.info(f"æ±‡æ€»ä¿¡æ¯å·²ä¿å­˜åˆ°: {summary_path}")


def process_multiple_files(input_files: List[str], output_dir: str, similarity_threshold: float = 0.90, use_gpu: bool = False, gpu_ids: List[int] = None) -> None:
    """å¤„ç†å¤šä¸ªè¾“å…¥æ–‡ä»¶ï¼ŒæŸ¥æ‰¾ä»£ç å’Œç¼–è¯‘é”™è¯¯åŸå› ç›¸ä¼¼çš„è¯­æ–™"""
    all_data = []
    file_sources = {}  # è®°å½•æ¯ä¸ªæ¡ç›®æ¥è‡ªå“ªä¸ªæ–‡ä»¶
    
    # åŠ è½½æ‰€æœ‰æ–‡ä»¶
    for file_path in input_files:
        data = load_json_file(file_path)
        file_name = os.path.basename(file_path)
        
        for item in data:
            all_data.append(item)
            file_sources[len(all_data) - 1] = file_name
    
    print(f"\nğŸ“ åŠ è½½å®Œæˆ: æ€»å…± {len(all_data):,} æ¡è¯­æ–™ï¼Œæ¥è‡ª {len(input_files)} ä¸ªæ–‡ä»¶")
    
    # åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ
    print(f"\nğŸ” ç¬¬ä¸€æ­¥: é‡‡æ ·åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ...")
    similarity_analysis = analyze_similarity_distribution(all_data)
    logger.info(f"ç›¸ä¼¼åº¦åˆ†æç»“æœ: å¹³å‡ç›¸ä¼¼åº¦ {similarity_analysis.get('avg_similarity', 0):.3f}, "
               f"é«˜ç›¸ä¼¼åº¦(>90%)è¯­æ–™æ•°: {similarity_analysis.get('high_similarity_count', 0)}, "
               f"æé«˜ç›¸ä¼¼åº¦(>95%)è¯­æ–™æ•°: {similarity_analysis.get('very_high_similarity_count', 0)}")
    
    # æŸ¥æ‰¾ç›¸ä¼¼ç»„
    print(f"\nğŸ” ç¬¬äºŒæ­¥: {'å¤šGPUå¹¶è¡Œ' if use_gpu else 'CPUä¼˜åŒ–'}åˆ†ææŸ¥æ‰¾ç›¸ä¼¼è¯­æ–™ç»„...")
    similar_groups = find_similar_groups(all_data, similarity_threshold, use_gpu, gpu_ids)
    
    if similar_groups:
        print(f"\nğŸ“Š åˆ†æç»“æœ: å‘ç° {len(similar_groups)} ä¸ªç›¸ä¼¼è¯­æ–™ç»„ï¼Œæ€»å…±åŒ…å« {sum(len(group) for group in similar_groups)} ä¸ªé‡å¤è¯­æ–™")
        
        # ä¿å­˜ç›¸ä¼¼ç»„
        base_name = "compile_errors"
        print(f"\nğŸ’¾ ç¬¬ä¸‰æ­¥: ä¿å­˜ç›¸ä¼¼è¯­æ–™ç»„åˆ°æ–‡ä»¶...")
        save_similar_groups(similar_groups, output_dir, base_name)
        
        # åˆ›å»ºç»Ÿè®¡æŠ¥å‘Š
        print(f"\nğŸ“ ç¬¬å››æ­¥: ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š...")
        create_analysis_report(all_data, similar_groups, similarity_analysis, output_dir, base_name)
        
        print(f"\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}")
        
    else:
        print(f"\nâŒ æœªå‘ç°ä»£ç å’Œç¼–è¯‘é”™è¯¯åŸå› ç›¸ä¼¼åº¦è¶…è¿‡ {similarity_threshold*100:.0f}% çš„è¯­æ–™ç»„")


def create_analysis_report(all_data: List[Dict], similar_groups: List[List[Dict]], 
                          similarity_analysis: Dict, output_dir: str, base_name: str) -> None:
    """åˆ›å»ºè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
    report = {
        'analysis_summary': {
            'total_records': len(all_data),
            'similar_groups_found': len(similar_groups),
            'total_duplicates': sum(len(group) for group in similar_groups),
            'duplicate_ratio': sum(len(group) for group in similar_groups) / len(all_data) if all_data else 0,
            'similarity_threshold_used': 0.90
        },
        'similarity_distribution': similarity_analysis,
        'error_type_analysis': {},
        'duplicate_groups_summary': []
    }
    
    # åˆ†æé”™è¯¯ç±»å‹åˆ†å¸ƒ
    error_types = {}
    for item in all_data:
        error_type = item.get('error_type', 'unknown')
        error_types[error_type] = error_types.get(error_type, 0) + 1
    
    report['error_type_analysis'] = {
        'distribution': error_types,
        'most_common': sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:10]
    }
    
    # åˆ†ææ¯ä¸ªç›¸ä¼¼ç»„
    for i, group in enumerate(similar_groups):
        group_summary = {
            'group_id': i + 1,
            'size': len(group),
            'error_type': group[0].get('error_type'),
            'error_type_detail': group[0].get('error_type_detail'),
            'uuids': [item.get('uuid') for item in group],
            'avg_diversity_score': sum(item.get('diversity_score', 0) for item in group) / len(group),
            'avg_retention_ratio': sum(item.get('retention_ratio', 0) for item in group) / len(group)
        }
        report['duplicate_groups_summary'].append(group_summary)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(output_dir, f"{base_name}_analysis_report.json")
    save_json_file(report, report_path)
    
    logger.info(f"è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


def main():
    parser = argparse.ArgumentParser(description='ç¼–è¯‘é”™è¯¯è¯­æ–™ç›¸ä¼¼æ€§åˆ†æå·¥å…· - æ‰¾å‡ºä»£ç å’Œç¼–è¯‘é”™è¯¯åŸå› 90%ä»¥ä¸Šç›¸ä¼¼çš„é‡å¤è¯­æ–™')
    parser.add_argument('input_files', nargs='+', help='è¾“å…¥JSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å¤šä¸ªæ–‡ä»¶ï¼‰')
    parser.add_argument('-o', '--output', default='./similar_errors_output', 
                       help='è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: ./similar_errors_outputï¼‰')
    parser.add_argument('-t', '--threshold', type=float, default=0.90,
                       help='ä»£ç å’Œç¼–è¯‘é”™è¯¯åŸå› ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤: 0.90ï¼Œå³90%ï¼‰')
    parser.add_argument('--analyze-only', action='store_true', help='ä»…åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒï¼Œä¸æŸ¥æ‰¾å…·ä½“çš„ç›¸ä¼¼ç»„')
    parser.add_argument('--gpu', action='store_true', help='ä½¿ç”¨GPUåŠ é€Ÿè®¡ç®—ï¼ˆéœ€è¦å®‰è£… torch å’Œ sentence-transformersï¼‰')
    parser.add_argument('--gpu-ids', type=str, default='0,1,2,3,4,5,6,7', 
                       help='æŒ‡å®šä½¿ç”¨çš„GPUå¡å·ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚: 0,1,2,3 (é»˜è®¤: 0,1,2,3,4,5,6,7)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for file_path in args.input_files:
        if not Path(file_path).exists():
            logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return
    
    # è§£æGPU IDs
    gpu_ids = None
    if args.gpu:
        try:
            gpu_ids = [int(x.strip()) for x in args.gpu_ids.split(',')]
            print(f"ğŸ¯ æŒ‡å®šGPUå¡: {gpu_ids}")
        except:
            print("âŒ GPUå¡å·æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
            gpu_ids = list(range(8))
        
        if check_gpu_availability():
            print("ğŸš€ GPUå¯ç”¨ï¼Œå°†ä½¿ç”¨å¤šGPUå¹¶è¡Œè®¡ç®—")
        else:
            print("âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®¡ç®—")
            print("   è¦ä½¿ç”¨GPUï¼Œè¯·å®‰è£…: pip install torch sentence-transformers")
            args.gpu = False
    
    try:
        if args.analyze_only:
            # ä»…åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ
            all_data = []
            for file_path in args.input_files:
                data = load_json_file(file_path)
                all_data.extend(data)
            
            logger.info(f"æ€»å…±åŠ è½½äº† {len(all_data)} æ¡è¯­æ–™")
            analysis = analyze_similarity_distribution(all_data)
            
            logger.info("\nä»£ç å’Œç¼–è¯‘é”™è¯¯åŸå› ç›¸ä¼¼åº¦åˆ†å¸ƒåˆ†æç»“æœ:")
            for key, value in analysis.items():
                if key != 'high_similarity_examples':
                    logger.info(f"  {key}: {value}")
            
            if 'high_similarity_examples' in analysis:
                logger.info("\né«˜ç›¸ä¼¼åº¦è¯­æ–™ç¤ºä¾‹:")
                for example in analysis['high_similarity_examples']:
                    logger.info(f"  {example['uuid1']} vs {example['uuid2']}: {example['similarity']:.3f}")
        else:
            # å®Œæ•´åˆ†æå¹¶ä¿å­˜ç›¸ä¼¼ç»„
            process_multiple_files(args.input_files, args.output, args.threshold, args.gpu, gpu_ids)
            
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 