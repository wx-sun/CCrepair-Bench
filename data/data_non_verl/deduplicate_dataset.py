#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†å»é‡è„šæœ¬
åŸºäºç›¸ä¼¼æ€§åˆ†æç»“æœï¼Œä»åŸå§‹æ•°æ®ä¸­ç§»é™¤é‡å¤é¡¹ï¼Œæ¯ä¸ªç›¸ä¼¼ç»„åªä¿ç•™ä¸€ä¸ªæ ·æœ¬
"""

import json
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any, Set
import os

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def load_json_file(file_path: str) -> List[Dict]:
    """åŠ è½½JSONæ–‡ä»¶"""
    try:
        logger.info(f"æ­£åœ¨åŠ è½½æ–‡ä»¶: {file_path}")
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            raise ValueError("JSONæ–‡ä»¶åº”è¯¥åŒ…å«ä¸€ä¸ªæ•°ç»„")
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(data)} æ¡è®°å½•")
        return data
    except Exception as e:
        logger.error(f"åŠ è½½JSONæ–‡ä»¶å¤±è´¥: {e}")
        raise


def save_json_file(data: List[Dict], file_path: str) -> None:
    """ä¿å­˜JSONæ–‡ä»¶"""
    try:
        logger.info(f"æ­£åœ¨ä¿å­˜æ–‡ä»¶: {file_path}")
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
    except Exception as e:
        logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
        raise


def load_similarity_summary(summary_path: str) -> Dict:
    """åŠ è½½ç›¸ä¼¼æ€§åˆ†ææ±‡æ€»æ–‡ä»¶"""
    try:
        logger.info(f"æ­£åœ¨åŠ è½½ç›¸ä¼¼æ€§åˆ†ææ±‡æ€»: {summary_path}")
        with open(summary_path, 'r', encoding='utf-8') as f:
            summary = json.load(f)
        
        logger.info(f"æ±‡æ€»ä¿¡æ¯: {summary['total_groups']} ä¸ªç›¸ä¼¼ç»„ï¼Œ{summary['total_duplicates']} ä¸ªé‡å¤é¡¹")
        return summary
    except Exception as e:
        logger.error(f"åŠ è½½ç›¸ä¼¼æ€§æ±‡æ€»æ–‡ä»¶å¤±è´¥: {e}")
        raise


def extract_duplicate_uuids(summary: Dict) -> Set[str]:
    """ä»æ±‡æ€»ä¿¡æ¯ä¸­æå–éœ€è¦åˆ é™¤çš„é‡å¤UUID"""
    uuids_to_remove = set()
    keep_first_uuids = set()
    
    for group_info in summary['groups_info']:
        uuids = group_info['uuids']
        if len(uuids) > 1:
            # ä¿ç•™ç¬¬ä¸€ä¸ªUUIDï¼Œåˆ é™¤å…¶ä½™çš„
            keep_first_uuids.add(uuids[0])
            for uuid in uuids[1:]:
                uuids_to_remove.add(uuid)
    
    logger.info(f"å°†ä¿ç•™ {len(keep_first_uuids)} ä¸ªä»£è¡¨æ€§æ ·æœ¬")
    logger.info(f"å°†åˆ é™¤ {len(uuids_to_remove)} ä¸ªé‡å¤æ ·æœ¬")
    
    return uuids_to_remove


def deduplicate_dataset(data: List[Dict], uuids_to_remove: Set[str]) -> List[Dict]:
    """ä»æ•°æ®é›†ä¸­ç§»é™¤é‡å¤é¡¹"""
    logger.info("å¼€å§‹å»é‡å¤„ç†...")
    
    original_count = len(data)
    deduplicated_data = []
    removed_count = 0
    
    for item in data:
        uuid = item.get('uuid', '')
        if uuid in uuids_to_remove:
            removed_count += 1
            logger.debug(f"åˆ é™¤é‡å¤é¡¹: {uuid}")
        else:
            deduplicated_data.append(item)
    
    final_count = len(deduplicated_data)
    logger.info(f"å»é‡å®Œæˆ:")
    logger.info(f"  åŸå§‹æ•°æ®: {original_count:,} æ¡")
    logger.info(f"  åˆ é™¤é‡å¤: {removed_count:,} æ¡")
    logger.info(f"  ä¿ç•™æ•°æ®: {final_count:,} æ¡")
    logger.info(f"  å»é‡ç‡: {(removed_count/original_count)*100:.2f}%")
    
    return deduplicated_data


def create_deduplication_report(original_count: int, final_count: int, removed_count: int, summary: Dict, output_path: str) -> None:
    """åˆ›å»ºå»é‡æŠ¥å‘Š"""
    report = {
        "deduplication_summary": {
            "original_count": original_count,
            "final_count": final_count,
            "removed_count": removed_count,
            "deduplication_rate": round((removed_count/original_count)*100, 2)
        },
        "similarity_analysis": {
            "total_similar_groups": summary['total_groups'],
            "total_duplicates_found": summary['total_duplicates'],
            "analysis_method": "TF-IDF + ä»£ç ç»“æ„åˆ†æ + difflib",
            "similarity_threshold": "90%"
        },
        "group_size_distribution": {}
    }
    
    # ç»Ÿè®¡ç›¸ä¼¼ç»„å¤§å°åˆ†å¸ƒ
    size_distribution = {}
    for group_info in summary['groups_info']:
        group_size = group_info['count']
        size_distribution[group_size] = size_distribution.get(group_size, 0) + 1
    
    report["group_size_distribution"] = size_distribution
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    logger.info(f"å»é‡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='æ•°æ®é›†å»é‡å·¥å…· - åŸºäºç›¸ä¼¼æ€§åˆ†æç»“æœå»é™¤é‡å¤é¡¹')
    parser.add_argument('input_file', help='è¾“å…¥çš„åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('-s', '--summary', required=True, 
                       help='ç›¸ä¼¼æ€§åˆ†ææ±‡æ€»æ–‡ä»¶è·¯å¾„ (compile_errors_similarity_summary.json)')
    parser.add_argument('-o', '--output', help='è¾“å‡ºçš„å»é‡æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤åœ¨è¾“å…¥æ–‡ä»¶åŒç›®å½•ä¸‹åˆ›å»ºï¼‰')
    parser.add_argument('--suffix', default='_deduplicated', 
                       help='è¾“å‡ºæ–‡ä»¶åç¼€ï¼ˆé»˜è®¤: _deduplicatedï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(args.input_file).exists():
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
        return
    
    if not Path(args.summary).exists():
        logger.error(f"ç›¸ä¼¼æ€§æ±‡æ€»æ–‡ä»¶ä¸å­˜åœ¨: {args.summary}")
        return
    
    try:
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if args.output:
            output_path = args.output
        else:
            input_path = Path(args.input_file)
            output_path = input_path.parent / f"{input_path.stem}{args.suffix}{input_path.suffix}"
        
        # åŠ è½½æ•°æ®
        original_data = load_json_file(args.input_file)
        summary = load_similarity_summary(args.summary)
        
        # æå–éœ€è¦åˆ é™¤çš„UUID
        uuids_to_remove = extract_duplicate_uuids(summary)
        
        # æ‰§è¡Œå»é‡
        deduplicated_data = deduplicate_dataset(original_data, uuids_to_remove)
        
        # ä¿å­˜å»é‡åçš„æ•°æ®
        save_json_file(deduplicated_data, str(output_path))
        
        # åˆ›å»ºå»é‡æŠ¥å‘Š
        report_path = str(output_path).replace('.json', '_deduplication_report.json')
        create_deduplication_report(
            len(original_data), 
            len(deduplicated_data), 
            len(original_data) - len(deduplicated_data),
            summary, 
            report_path
        )
        
        print(f"\nâœ… å»é‡å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - åŸå§‹æ•°æ®: {len(original_data):,} æ¡")
        print(f"   - å»é‡åæ•°æ®: {len(deduplicated_data):,} æ¡")
        print(f"   - åˆ é™¤é‡å¤: {len(original_data) - len(deduplicated_data):,} æ¡")
        print(f"   - å»é‡ç‡: {((len(original_data) - len(deduplicated_data))/len(original_data))*100:.2f}%")
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
        print(f"   - å»é‡æ•°æ®: {output_path}")
        print(f"   - å»é‡æŠ¥å‘Š: {report_path}")
        
    except Exception as e:
        logger.error(f"å»é‡å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 